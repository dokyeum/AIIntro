{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c795302-801f-49ec-9696-0923ffac7c35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.299471978148745\n",
      "=== epoch:1, train acc:0.165, test acc:0.175 ===\n",
      "train loss:2.2966417533021817\n",
      "train loss:2.2921468003710954\n",
      "train loss:2.2909465123231016\n",
      "train loss:2.277617743436498\n",
      "train loss:2.2741003349296602\n",
      "train loss:2.266938518138082\n",
      "train loss:2.2387949328955874\n",
      "train loss:2.2381761989117623\n",
      "train loss:2.2130669718904166\n",
      "train loss:2.1621487860605497\n",
      "train loss:2.15292274362697\n",
      "train loss:2.1022048112349867\n",
      "train loss:2.052592777210455\n",
      "train loss:1.9869860621203745\n",
      "train loss:1.960454519647226\n",
      "train loss:1.8950277833924987\n",
      "train loss:1.803500759096284\n",
      "train loss:1.7339741210098407\n",
      "train loss:1.7345438747581352\n",
      "train loss:1.5806934504402002\n",
      "train loss:1.5830667819048774\n",
      "train loss:1.48086764672963\n",
      "train loss:1.40665748714456\n",
      "train loss:1.305718009654911\n",
      "train loss:1.2791883138435272\n",
      "train loss:1.1203244333018088\n",
      "train loss:1.0950246073462322\n",
      "train loss:1.0368544838407052\n",
      "train loss:0.8755425129800546\n",
      "train loss:0.8419604231547865\n",
      "train loss:1.0152360197981307\n",
      "train loss:0.8514058808398862\n",
      "train loss:0.8381998630347013\n",
      "train loss:0.8207113936019096\n",
      "train loss:0.7447662647352703\n",
      "train loss:0.7573913357445319\n",
      "train loss:0.4981463580801624\n",
      "train loss:0.8081086784383054\n",
      "train loss:0.6522483999007906\n",
      "train loss:0.7199556798143685\n",
      "train loss:0.6144711728581667\n",
      "train loss:0.5470649064498827\n",
      "train loss:0.65044137864479\n",
      "train loss:0.5282428429948184\n",
      "train loss:0.5787810649762831\n",
      "train loss:0.4891789071001741\n",
      "train loss:0.4252586075844411\n",
      "train loss:0.38396135252178404\n",
      "train loss:0.4019725502132658\n",
      "train loss:0.6543555416139561\n",
      "=== epoch:2, train acc:0.804, test acc:0.772 ===\n",
      "train loss:0.6756556930614522\n",
      "train loss:0.5173944385966656\n",
      "train loss:0.45878721584930354\n",
      "train loss:0.5013932186727624\n",
      "train loss:0.5440191721423162\n",
      "train loss:0.43579709290311286\n",
      "train loss:0.4527072019074723\n",
      "train loss:0.5194190685347023\n",
      "train loss:0.4103947236000898\n",
      "train loss:0.42177705626231415\n",
      "train loss:0.5458599353475608\n",
      "train loss:0.4868965106286401\n",
      "train loss:0.47678940933629865\n",
      "train loss:0.32956816780063336\n",
      "train loss:0.5555019371174315\n",
      "train loss:0.5478099064982666\n",
      "train loss:0.5336831266357205\n",
      "train loss:0.6029862581562814\n",
      "train loss:0.4906145577240671\n",
      "train loss:0.5180359395819977\n",
      "train loss:0.3639504194333935\n",
      "train loss:0.31587655645290263\n",
      "train loss:0.3107190406821172\n",
      "train loss:0.37611335337503976\n",
      "train loss:0.4699112440055277\n",
      "train loss:0.3796082040802648\n",
      "train loss:0.5270746100543482\n",
      "train loss:0.3623850785622271\n",
      "train loss:0.19746182813387209\n",
      "train loss:0.3342614304455783\n",
      "train loss:0.2913168790900658\n",
      "train loss:0.4091710117511019\n",
      "train loss:0.514409079560096\n",
      "train loss:0.4506317056787246\n",
      "train loss:0.45082346858055367\n",
      "train loss:0.3918933181422817\n",
      "train loss:0.38216236340389736\n",
      "train loss:0.42798997158314345\n",
      "train loss:0.33907014801275587\n",
      "train loss:0.5153570457907669\n",
      "train loss:0.40083479655492477\n",
      "train loss:0.29147274459387246\n",
      "train loss:0.3679226432877582\n",
      "train loss:0.262482877115022\n",
      "train loss:0.44127287581153135\n",
      "train loss:0.43311184550813164\n",
      "train loss:0.3268039293697633\n",
      "train loss:0.33881548710273196\n",
      "train loss:0.21748532829603495\n",
      "train loss:0.37596849031087715\n",
      "=== epoch:3, train acc:0.885, test acc:0.853 ===\n",
      "train loss:0.3834214739440079\n",
      "train loss:0.26539685503555555\n",
      "train loss:0.3427945449840654\n",
      "train loss:0.4971287011953215\n",
      "train loss:0.30576578639275054\n",
      "train loss:0.2554488998413317\n",
      "train loss:0.4385287386898407\n",
      "train loss:0.4066818852429044\n",
      "train loss:0.40369265725294246\n",
      "train loss:0.6282758490288279\n",
      "train loss:0.4642079927266458\n",
      "train loss:0.3701497824960488\n",
      "train loss:0.28071091634698414\n",
      "train loss:0.27049450873618475\n",
      "train loss:0.4879017203466372\n",
      "train loss:0.3175229631908076\n",
      "train loss:0.3502385530217311\n",
      "train loss:0.4088055602661204\n",
      "train loss:0.3599341932021432\n",
      "train loss:0.34601190416611666\n",
      "train loss:0.30801785618331007\n",
      "train loss:0.28460127747598013\n",
      "train loss:0.2999468155574593\n",
      "train loss:0.2504880016636008\n",
      "train loss:0.35074280713046263\n",
      "train loss:0.41736703803215897\n",
      "train loss:0.2897311608319502\n",
      "train loss:0.3255445801873428\n",
      "train loss:0.28942253501556936\n",
      "train loss:0.24822688043243182\n",
      "train loss:0.44823471889197575\n",
      "train loss:0.29808119415660644\n",
      "train loss:0.3027366318531092\n",
      "train loss:0.3445047842499648\n",
      "train loss:0.1950966197744174\n",
      "train loss:0.2567346785351422\n",
      "train loss:0.1940709015651036\n",
      "train loss:0.19317574418096142\n",
      "train loss:0.22910334038345972\n",
      "train loss:0.296883908978366\n",
      "train loss:0.31992178417261835\n",
      "train loss:0.2894264413521024\n",
      "train loss:0.31547815116904987\n",
      "train loss:0.33268692705818625\n",
      "train loss:0.23977215051096606\n",
      "train loss:0.3974865637963131\n",
      "train loss:0.26650089603274574\n",
      "train loss:0.29017881102534754\n",
      "train loss:0.25679984576501413\n",
      "train loss:0.2034926727658773\n",
      "=== epoch:4, train acc:0.901, test acc:0.883 ===\n",
      "train loss:0.2790185580211349\n",
      "train loss:0.2381305498104417\n",
      "train loss:0.20851878350891137\n",
      "train loss:0.2424909738591334\n",
      "train loss:0.22515926100373893\n",
      "train loss:0.16751869575320588\n",
      "train loss:0.2577158246169143\n",
      "train loss:0.32717797888463884\n",
      "train loss:0.18999006210424485\n",
      "train loss:0.3408231814472641\n",
      "train loss:0.21374316777913493\n",
      "train loss:0.45203466751551\n",
      "train loss:0.26123116646046574\n",
      "train loss:0.21888940498807055\n",
      "train loss:0.20084376219331923\n",
      "train loss:0.3044414527940313\n",
      "train loss:0.29564889107104786\n",
      "train loss:0.35387744475506766\n",
      "train loss:0.2239464415823997\n",
      "train loss:0.29206283021512197\n",
      "train loss:0.2850545266419433\n",
      "train loss:0.19088035499208747\n",
      "train loss:0.25353894569202917\n",
      "train loss:0.4233433054983438\n",
      "train loss:0.33560837346363726\n",
      "train loss:0.2646060655310673\n",
      "train loss:0.25782531674463116\n",
      "train loss:0.22051314265339933\n",
      "train loss:0.30554933140682383\n",
      "train loss:0.18565323707917\n",
      "train loss:0.24645116646100326\n",
      "train loss:0.3313050867856274\n",
      "train loss:0.49027286960032823\n",
      "train loss:0.4563980489554816\n",
      "train loss:0.20696939711259396\n",
      "train loss:0.20555960175688692\n",
      "train loss:0.2238756907762274\n",
      "train loss:0.3683393046710763\n",
      "train loss:0.21716248306131247\n",
      "train loss:0.2092352413199575\n",
      "train loss:0.1897327338404021\n",
      "train loss:0.289298358749098\n",
      "train loss:0.24030654849393845\n",
      "train loss:0.11836841305791429\n",
      "train loss:0.31753277731606533\n",
      "train loss:0.12608684937541553\n",
      "train loss:0.2078941377140309\n",
      "train loss:0.23559780784303122\n",
      "train loss:0.1898459123602763\n",
      "train loss:0.20371192806779756\n",
      "=== epoch:5, train acc:0.92, test acc:0.896 ===\n",
      "train loss:0.1968580359325136\n",
      "train loss:0.24479227989326233\n",
      "train loss:0.18428001923682646\n",
      "train loss:0.31299023588208813\n",
      "train loss:0.2078056290910025\n",
      "train loss:0.1765076209490529\n",
      "train loss:0.3022720653379743\n",
      "train loss:0.17537727230777989\n",
      "train loss:0.30972488310264795\n",
      "train loss:0.28647662071130514\n",
      "train loss:0.1510447309222573\n",
      "train loss:0.2436789680926655\n",
      "train loss:0.19449109435061526\n",
      "train loss:0.20059509107409823\n",
      "train loss:0.12901356571842473\n",
      "train loss:0.12455342716023254\n",
      "train loss:0.15954705697006377\n",
      "train loss:0.32188372895521555\n",
      "train loss:0.20524485393901176\n",
      "train loss:0.1921259651329597\n",
      "train loss:0.2598809642320241\n",
      "train loss:0.2620986859079114\n",
      "train loss:0.12982420208154122\n",
      "train loss:0.25573180470629714\n",
      "train loss:0.2373726600027559\n",
      "train loss:0.17326781633701485\n",
      "train loss:0.25240203349880713\n",
      "train loss:0.1708247922317818\n",
      "train loss:0.3480951186261086\n",
      "train loss:0.28148033000722283\n",
      "train loss:0.17983228123028572\n",
      "train loss:0.18850415265622142\n",
      "train loss:0.15692981242843554\n",
      "train loss:0.2983611456992739\n",
      "train loss:0.22998022331775167\n",
      "train loss:0.23946868737834306\n",
      "train loss:0.18309951900159635\n",
      "train loss:0.24032354441611298\n",
      "train loss:0.3243208084528405\n",
      "train loss:0.15555524721636993\n",
      "train loss:0.21158656493943132\n",
      "train loss:0.3303064056015515\n",
      "train loss:0.2416064114345942\n",
      "train loss:0.20931281294723963\n",
      "train loss:0.18487739344650694\n",
      "train loss:0.21112673204280305\n",
      "train loss:0.1874271239620505\n",
      "train loss:0.297785362294418\n",
      "train loss:0.1295253231164436\n",
      "train loss:0.3192403065853592\n",
      "=== epoch:6, train acc:0.932, test acc:0.903 ===\n",
      "train loss:0.13769480606039625\n",
      "train loss:0.135930081805488\n",
      "train loss:0.28775990356861025\n",
      "train loss:0.13821761937227448\n",
      "train loss:0.16970689328054853\n",
      "train loss:0.15143229013503348\n",
      "train loss:0.23560824119540066\n",
      "train loss:0.2041100977549296\n",
      "train loss:0.19790414419130634\n",
      "train loss:0.1626283785334201\n",
      "train loss:0.1897148334393077\n",
      "train loss:0.2633057667915338\n",
      "train loss:0.20873900458861516\n",
      "train loss:0.1905695727331452\n",
      "train loss:0.30224824404662715\n",
      "train loss:0.25595401484674024\n",
      "train loss:0.09961188045000656\n",
      "train loss:0.2313327924932166\n",
      "train loss:0.11825285068191539\n",
      "train loss:0.10300725486916874\n",
      "train loss:0.22560235880782561\n",
      "train loss:0.14296239343698233\n",
      "train loss:0.12651607880757432\n",
      "train loss:0.24404459774001694\n",
      "train loss:0.17980702266875234\n",
      "train loss:0.17195626392140237\n",
      "train loss:0.12266853010135778\n",
      "train loss:0.09297775760628153\n",
      "train loss:0.24460138538705045\n",
      "train loss:0.11397555229003094\n",
      "train loss:0.21706952329341875\n",
      "train loss:0.3124668123234312\n",
      "train loss:0.20076959428156357\n",
      "train loss:0.15688906089359775\n",
      "train loss:0.15234184578149648\n",
      "train loss:0.1838409713125471\n",
      "train loss:0.2549347266264702\n",
      "train loss:0.0701953989076309\n",
      "train loss:0.16878054892118802\n",
      "train loss:0.1341600713280325\n",
      "train loss:0.23420478354750035\n",
      "train loss:0.14615598979124378\n",
      "train loss:0.10958486193962978\n",
      "train loss:0.19172200065115855\n",
      "train loss:0.2933616008017797\n",
      "train loss:0.22767504466080268\n",
      "train loss:0.13013204856369293\n",
      "train loss:0.09984749718969299\n",
      "train loss:0.2849722900830955\n",
      "train loss:0.12322983549956382\n",
      "=== epoch:7, train acc:0.947, test acc:0.912 ===\n",
      "train loss:0.13091573935989664\n",
      "train loss:0.1316226493556173\n",
      "train loss:0.24930457229809247\n",
      "train loss:0.10582684394254452\n",
      "train loss:0.19788791015998303\n",
      "train loss:0.18751922942229432\n",
      "train loss:0.1770977412049404\n",
      "train loss:0.15467807378137333\n",
      "train loss:0.14447801862019843\n",
      "train loss:0.12845948871905408\n",
      "train loss:0.19906350838327783\n",
      "train loss:0.17791584587713924\n",
      "train loss:0.22552877641784366\n",
      "train loss:0.3169580429755109\n",
      "train loss:0.15250623492536522\n",
      "train loss:0.35137349581627997\n",
      "train loss:0.23447312823706168\n",
      "train loss:0.3018031538228313\n",
      "train loss:0.09316408303177912\n",
      "train loss:0.07763509177454415\n",
      "train loss:0.11381731208181604\n",
      "train loss:0.15897329867953996\n",
      "train loss:0.22328066206597266\n",
      "train loss:0.17932403559041163\n",
      "train loss:0.14928603707014046\n",
      "train loss:0.07652131702169775\n",
      "train loss:0.12304022965009594\n",
      "train loss:0.14200918734016457\n",
      "train loss:0.16002987688464995\n",
      "train loss:0.10017308233489203\n",
      "train loss:0.12586231642910795\n",
      "train loss:0.14692736056107425\n",
      "train loss:0.13836274995660502\n",
      "train loss:0.1223280479942096\n",
      "train loss:0.11647262909898463\n",
      "train loss:0.09642270280998541\n",
      "train loss:0.0556954821214596\n",
      "train loss:0.17508418368510995\n",
      "train loss:0.16564197751508764\n",
      "train loss:0.150790698216908\n",
      "train loss:0.11929090433495915\n",
      "train loss:0.18973939817176494\n",
      "train loss:0.17651624826132972\n",
      "train loss:0.22733131176953414\n",
      "train loss:0.15241762499939865\n",
      "train loss:0.2350549908388107\n",
      "train loss:0.0907297275402024\n",
      "train loss:0.20005917515991734\n",
      "train loss:0.16621301030865424\n",
      "train loss:0.12885180433188587\n",
      "=== epoch:8, train acc:0.942, test acc:0.928 ===\n",
      "train loss:0.1457100472572208\n",
      "train loss:0.10275748027359803\n",
      "train loss:0.12101412609151901\n",
      "train loss:0.11959642869157834\n",
      "train loss:0.09300971215935638\n",
      "train loss:0.09115187217419955\n",
      "train loss:0.25310939375678443\n",
      "train loss:0.12347488738143632\n",
      "train loss:0.2832181813953289\n",
      "train loss:0.08992804532023616\n",
      "train loss:0.18500241209972082\n",
      "train loss:0.09218405239070691\n",
      "train loss:0.11760151144819134\n",
      "train loss:0.1458847654379011\n",
      "train loss:0.13957573915154758\n",
      "train loss:0.15378688239508886\n",
      "train loss:0.12071161356182525\n",
      "train loss:0.312103556958949\n",
      "train loss:0.2529631552436376\n",
      "train loss:0.09591146770205969\n",
      "train loss:0.15836667673680518\n",
      "train loss:0.09409255120768395\n",
      "train loss:0.1626185004961784\n",
      "train loss:0.13281286521037583\n",
      "train loss:0.1885540520237428\n",
      "train loss:0.11501527707082441\n",
      "train loss:0.10489148538471785\n",
      "train loss:0.17207164327304586\n",
      "train loss:0.15942891771202647\n",
      "train loss:0.1630507584215474\n",
      "train loss:0.08939458466840028\n",
      "train loss:0.07567784875086533\n",
      "train loss:0.14586459784726308\n",
      "train loss:0.051791162118165174\n",
      "train loss:0.16855334911564968\n",
      "train loss:0.15427429576203297\n",
      "train loss:0.08853977497131005\n",
      "train loss:0.18539093590304692\n",
      "train loss:0.10132831574279276\n",
      "train loss:0.07414273023514563\n",
      "train loss:0.11030057553321111\n",
      "train loss:0.2289518214079621\n",
      "train loss:0.18981691217955696\n",
      "train loss:0.2073876894702356\n",
      "train loss:0.07382325063137288\n",
      "train loss:0.06630814753232411\n",
      "train loss:0.06821671441066651\n",
      "train loss:0.11928026454983759\n",
      "train loss:0.12155181978592935\n",
      "train loss:0.169299505083709\n",
      "=== epoch:9, train acc:0.953, test acc:0.928 ===\n",
      "train loss:0.16600885113823538\n",
      "train loss:0.1887895200033256\n",
      "train loss:0.1038504933155668\n",
      "train loss:0.10653285664874808\n",
      "train loss:0.21897918533673674\n",
      "train loss:0.12507101285117245\n",
      "train loss:0.15921228886396216\n",
      "train loss:0.20135636256164516\n",
      "train loss:0.16899738387597726\n",
      "train loss:0.17079410031616843\n",
      "train loss:0.12989511355833847\n",
      "train loss:0.182466609078368\n",
      "train loss:0.057751375879654156\n",
      "train loss:0.0858967937709247\n",
      "train loss:0.10582102499277177\n",
      "train loss:0.10889118871455283\n",
      "train loss:0.053531856210331935\n",
      "train loss:0.10957462122532503\n",
      "train loss:0.11082068280306297\n",
      "train loss:0.19658278127767584\n",
      "train loss:0.09729009339373025\n",
      "train loss:0.15567778964308307\n",
      "train loss:0.059384763718148055\n",
      "train loss:0.14644906852608103\n",
      "train loss:0.1274969120684898\n",
      "train loss:0.10160465842603171\n",
      "train loss:0.06552501275105677\n",
      "train loss:0.08873514179348505\n",
      "train loss:0.0574840213897184\n",
      "train loss:0.1402264784037127\n",
      "train loss:0.10273033443066305\n",
      "train loss:0.12402837932321549\n",
      "train loss:0.1574553429623171\n",
      "train loss:0.19484324353494528\n",
      "train loss:0.18887500929334422\n",
      "train loss:0.20994919191775374\n",
      "train loss:0.09891751534208003\n",
      "train loss:0.08115738567656942\n",
      "train loss:0.16388992910953895\n",
      "train loss:0.06453085500868236\n",
      "train loss:0.05340691515665722\n",
      "train loss:0.05473817844684069\n",
      "train loss:0.13097264576300544\n",
      "train loss:0.07149407373338695\n",
      "train loss:0.07718074449442831\n",
      "train loss:0.09940346376554478\n",
      "train loss:0.06223021104788127\n",
      "train loss:0.08894592380512804\n",
      "train loss:0.14464523412746588\n",
      "train loss:0.11685920745238229\n",
      "=== epoch:10, train acc:0.966, test acc:0.933 ===\n",
      "train loss:0.10964610284761517\n",
      "train loss:0.12468514878892079\n",
      "train loss:0.17578156248883942\n",
      "train loss:0.03056468283189585\n",
      "train loss:0.09593733965666691\n",
      "train loss:0.045349625728369446\n",
      "train loss:0.0808720022764579\n",
      "train loss:0.04806526754082409\n",
      "train loss:0.05177422705124716\n",
      "train loss:0.08354452180326559\n",
      "train loss:0.12401024735254614\n",
      "train loss:0.05603552585554257\n",
      "train loss:0.07865539909161193\n",
      "train loss:0.12563813263213824\n",
      "train loss:0.2551336497091166\n",
      "train loss:0.030620273876258333\n",
      "train loss:0.11677178116262246\n",
      "train loss:0.12519546578781499\n",
      "train loss:0.0667642029347466\n",
      "train loss:0.044980453167856434\n",
      "train loss:0.07516265502433563\n",
      "train loss:0.08994053254575399\n",
      "train loss:0.04592566961653052\n",
      "train loss:0.13292097548226528\n",
      "train loss:0.03630602547835551\n",
      "train loss:0.11308868400809237\n",
      "train loss:0.07613213323427939\n",
      "train loss:0.05162794426560973\n",
      "train loss:0.12442641692828652\n",
      "train loss:0.04680636995457944\n",
      "train loss:0.19094132312563306\n",
      "train loss:0.12923103267536928\n",
      "train loss:0.12163469749763806\n",
      "train loss:0.07739004943541067\n",
      "train loss:0.062208721829456436\n",
      "train loss:0.15322258918153786\n",
      "train loss:0.08713289174942568\n",
      "train loss:0.07148896793875187\n",
      "train loss:0.05206598501658757\n",
      "train loss:0.08670811665514146\n",
      "train loss:0.23847590497627824\n",
      "train loss:0.04246876077878966\n",
      "train loss:0.09655508545915206\n",
      "train loss:0.06933162519335101\n",
      "train loss:0.08838492237863649\n",
      "train loss:0.15422750940539015\n",
      "train loss:0.09681412277407268\n",
      "train loss:0.08639779406800413\n",
      "train loss:0.10248884911857115\n",
      "train loss:0.06203251779316274\n",
      "=== epoch:11, train acc:0.97, test acc:0.944 ===\n",
      "train loss:0.06091313805659199\n",
      "train loss:0.31136305735694725\n",
      "train loss:0.10418796071408601\n",
      "train loss:0.20203805345781606\n",
      "train loss:0.051385369766453824\n",
      "train loss:0.05850177324231444\n",
      "train loss:0.13708938197334464\n",
      "train loss:0.09104205572286865\n",
      "train loss:0.09727467515788076\n",
      "train loss:0.09292893961787745\n",
      "train loss:0.09690385639754934\n",
      "train loss:0.06344648628050317\n",
      "train loss:0.09576290904040441\n",
      "train loss:0.04699826866076015\n",
      "train loss:0.06788653961773683\n",
      "train loss:0.09205298494031826\n",
      "train loss:0.08731300233776773\n",
      "train loss:0.0571915094746777\n",
      "train loss:0.07470505892508332\n",
      "train loss:0.10605430572803021\n",
      "train loss:0.09793922248137805\n",
      "train loss:0.07087816103419706\n",
      "train loss:0.09814859172453679\n",
      "train loss:0.1650407302039298\n",
      "train loss:0.0655506444700987\n",
      "train loss:0.044469128887124044\n",
      "train loss:0.12070165238059345\n",
      "train loss:0.053636003814378365\n",
      "train loss:0.12426684006743816\n",
      "train loss:0.04941801870248787\n",
      "train loss:0.048740607202512526\n",
      "train loss:0.09604718505954922\n",
      "train loss:0.0970405140916352\n",
      "train loss:0.13859111807675178\n",
      "train loss:0.14761710919374516\n",
      "train loss:0.05595234229766408\n",
      "train loss:0.0402906933383316\n",
      "train loss:0.19543754791426132\n",
      "train loss:0.09614253846994077\n",
      "train loss:0.09622350137119497\n",
      "train loss:0.05488016711025843\n",
      "train loss:0.1243595533176011\n",
      "train loss:0.1139622176093147\n",
      "train loss:0.13075515699125653\n",
      "train loss:0.08018140051485727\n",
      "train loss:0.08871736450786155\n",
      "train loss:0.08140461784939934\n",
      "train loss:0.10040512308935347\n",
      "train loss:0.074289364859809\n",
      "train loss:0.10062923996187417\n",
      "=== epoch:12, train acc:0.967, test acc:0.938 ===\n",
      "train loss:0.08181790465702173\n",
      "train loss:0.056808953876957995\n",
      "train loss:0.05543047346205365\n",
      "train loss:0.10865520286436027\n",
      "train loss:0.06526272008372938\n",
      "train loss:0.0728921823953882\n",
      "train loss:0.0534444781455835\n",
      "train loss:0.1385928396959896\n",
      "train loss:0.09838577583598196\n",
      "train loss:0.07144531008432552\n",
      "train loss:0.07922302401715264\n",
      "train loss:0.0574145320447543\n",
      "train loss:0.08842853525262839\n",
      "train loss:0.06735622294132147\n",
      "train loss:0.0517395908159117\n",
      "train loss:0.10384189480247287\n",
      "train loss:0.06497250143374585\n",
      "train loss:0.05960652211955035\n",
      "train loss:0.07245515938457495\n",
      "train loss:0.062377664516603086\n",
      "train loss:0.045617991455332935\n",
      "train loss:0.0856826127094373\n",
      "train loss:0.036384953824359474\n",
      "train loss:0.06971412013460411\n",
      "train loss:0.0955779005596818\n",
      "train loss:0.08113078722663207\n",
      "train loss:0.024892401620373854\n",
      "train loss:0.06673982299418432\n",
      "train loss:0.13025931564466942\n",
      "train loss:0.0470829228413089\n",
      "train loss:0.037702321510708355\n",
      "train loss:0.05431405807168208\n",
      "train loss:0.14299013609013084\n",
      "train loss:0.08718553980633144\n",
      "train loss:0.07589747749957634\n",
      "train loss:0.03801828323980582\n",
      "train loss:0.19689540275657832\n",
      "train loss:0.05586635021521046\n",
      "train loss:0.11788882012638734\n",
      "train loss:0.029901259428540307\n",
      "train loss:0.04325488341576927\n",
      "train loss:0.04438184995836622\n",
      "train loss:0.03676830054279855\n",
      "train loss:0.04707220309225476\n",
      "train loss:0.1438326951975528\n",
      "train loss:0.06028178419740242\n",
      "train loss:0.08495825737987302\n",
      "train loss:0.036248381695584767\n",
      "train loss:0.05045555669834527\n",
      "train loss:0.03127758815700936\n",
      "=== epoch:13, train acc:0.972, test acc:0.942 ===\n",
      "train loss:0.0453934531482148\n",
      "train loss:0.07107468682157231\n",
      "train loss:0.05134093928912111\n",
      "train loss:0.08593504819720225\n",
      "train loss:0.11048669050038243\n",
      "train loss:0.0618647801371795\n",
      "train loss:0.06819102497461961\n",
      "train loss:0.12706970547501625\n",
      "train loss:0.0782328757696915\n",
      "train loss:0.05621386419447636\n",
      "train loss:0.05368967962169435\n",
      "train loss:0.053736577750807\n",
      "train loss:0.04073881087391969\n",
      "train loss:0.03086947306543073\n",
      "train loss:0.06897155650791581\n",
      "train loss:0.09699574408822104\n",
      "train loss:0.04269713151366927\n",
      "train loss:0.05577331395967684\n",
      "train loss:0.03733234727677088\n",
      "train loss:0.0740944340636592\n",
      "train loss:0.09148892720057007\n",
      "train loss:0.08657070910277004\n",
      "train loss:0.07703410731462623\n",
      "train loss:0.036086933761109566\n",
      "train loss:0.05365212472041904\n",
      "train loss:0.06275592907958971\n",
      "train loss:0.05868522198637039\n",
      "train loss:0.03922835205526547\n",
      "train loss:0.023988795037948252\n",
      "train loss:0.06414421420065777\n",
      "train loss:0.08778020316083669\n",
      "train loss:0.07418508273682259\n",
      "train loss:0.05906201546622165\n",
      "train loss:0.06820304715870851\n",
      "train loss:0.14007649956955065\n",
      "train loss:0.05527877985394787\n",
      "train loss:0.1717834078531858\n",
      "train loss:0.12850791082640176\n",
      "train loss:0.07708364843495402\n",
      "train loss:0.06797740360024992\n",
      "train loss:0.10024806730739425\n",
      "train loss:0.03951719515590653\n",
      "train loss:0.06531183458147982\n",
      "train loss:0.12908276365055457\n",
      "train loss:0.060123931769619936\n",
      "train loss:0.07172268801690532\n",
      "train loss:0.07456000406471762\n",
      "train loss:0.14147376837054237\n",
      "train loss:0.08667659783613017\n",
      "train loss:0.03503256425316758\n",
      "=== epoch:14, train acc:0.974, test acc:0.945 ===\n",
      "train loss:0.10876504542826758\n",
      "train loss:0.07222167136036915\n",
      "train loss:0.11607857399671062\n",
      "train loss:0.04522327133489537\n",
      "train loss:0.14392097975659587\n",
      "train loss:0.03780532368980935\n",
      "train loss:0.13455346463926146\n",
      "train loss:0.1076468903160357\n",
      "train loss:0.1776004278873469\n",
      "train loss:0.06932214315331153\n",
      "train loss:0.06722022456503082\n",
      "train loss:0.11055363291022494\n",
      "train loss:0.03314606012784898\n",
      "train loss:0.035012677851405966\n",
      "train loss:0.06803443503955013\n",
      "train loss:0.0749636540216792\n",
      "train loss:0.05963214486112926\n",
      "train loss:0.053048928691834266\n",
      "train loss:0.02979414903319227\n",
      "train loss:0.03222461663957506\n",
      "train loss:0.06765371804854427\n",
      "train loss:0.10293228081482164\n",
      "train loss:0.025657656077367877\n",
      "train loss:0.04374886306137091\n",
      "train loss:0.04749052353249448\n",
      "train loss:0.036067248161467474\n",
      "train loss:0.03272497421734995\n",
      "train loss:0.06925727627655288\n",
      "train loss:0.04075879593955101\n",
      "train loss:0.04679379180127564\n",
      "train loss:0.04454904995078055\n",
      "train loss:0.08959036293862767\n",
      "train loss:0.048559951134579\n",
      "train loss:0.09054650413803583\n",
      "train loss:0.034782623074857036\n",
      "train loss:0.12735889026283\n",
      "train loss:0.08257757140931683\n",
      "train loss:0.060224440644325944\n",
      "train loss:0.05993113878199799\n",
      "train loss:0.06811450517174782\n",
      "train loss:0.09125754960994838\n",
      "train loss:0.01998752854331749\n",
      "train loss:0.06973156820023971\n",
      "train loss:0.07422484692629426\n",
      "train loss:0.07047446821166217\n",
      "train loss:0.06968655636432436\n",
      "train loss:0.04875070751594778\n",
      "train loss:0.06908697065609261\n",
      "train loss:0.03459890831577765\n",
      "train loss:0.07312360668638651\n",
      "=== epoch:15, train acc:0.978, test acc:0.953 ===\n",
      "train loss:0.05936325143418817\n",
      "train loss:0.020570442248879068\n",
      "train loss:0.035250445619222165\n",
      "train loss:0.03977892262470531\n",
      "train loss:0.06179821572714102\n",
      "train loss:0.06932065542511442\n",
      "train loss:0.04590949343511975\n",
      "train loss:0.03067760359796233\n",
      "train loss:0.03556990039054398\n",
      "train loss:0.053048523281922604\n",
      "train loss:0.03658832367091971\n",
      "train loss:0.03532503957831651\n",
      "train loss:0.03465230112789896\n",
      "train loss:0.03724128645842998\n",
      "train loss:0.09183939785652555\n",
      "train loss:0.06368516859443668\n",
      "train loss:0.024926155042609678\n",
      "train loss:0.03231807844146952\n",
      "train loss:0.06732532572571442\n",
      "train loss:0.05276291200566452\n",
      "train loss:0.040169128360560344\n",
      "train loss:0.08204241600179714\n",
      "train loss:0.03618447333848267\n",
      "train loss:0.03458413095353229\n",
      "train loss:0.017561643040673716\n",
      "train loss:0.0331008354593408\n",
      "train loss:0.04878495453534648\n",
      "train loss:0.019086967367828552\n",
      "train loss:0.07343525419513244\n",
      "train loss:0.07757740938541462\n",
      "train loss:0.09163143468840129\n",
      "train loss:0.07061170409629745\n",
      "train loss:0.05746129657819363\n",
      "train loss:0.09449457814563861\n",
      "train loss:0.07281843480863329\n",
      "train loss:0.03212180819830083\n",
      "train loss:0.028165091067008846\n",
      "train loss:0.055686648632442506\n",
      "train loss:0.05125021947410566\n",
      "train loss:0.024910226245926897\n",
      "train loss:0.06293463093990335\n",
      "train loss:0.07727370350238365\n",
      "train loss:0.06862796482646215\n",
      "train loss:0.02254973474765352\n",
      "train loss:0.03777057677419071\n",
      "train loss:0.036259497821193504\n",
      "train loss:0.027228321498803183\n",
      "train loss:0.024421991799705473\n",
      "train loss:0.09095882358225175\n",
      "train loss:0.031266744999654525\n",
      "=== epoch:16, train acc:0.982, test acc:0.953 ===\n",
      "train loss:0.06896533165392733\n",
      "train loss:0.042830274475616964\n",
      "train loss:0.03265371143545772\n",
      "train loss:0.015266392831304564\n",
      "train loss:0.046007409853928136\n",
      "train loss:0.02875979742793746\n",
      "train loss:0.05147499134641326\n",
      "train loss:0.03961322241484341\n",
      "train loss:0.045452911719735895\n",
      "train loss:0.037470436804292455\n",
      "train loss:0.043096507547983956\n",
      "train loss:0.022438103402631825\n",
      "train loss:0.048706937468326404\n",
      "train loss:0.03338251979573358\n",
      "train loss:0.04572430712557075\n",
      "train loss:0.04048586694033306\n",
      "train loss:0.032382994825326514\n",
      "train loss:0.11712776863508643\n",
      "train loss:0.033695505222426314\n",
      "train loss:0.05289041988057457\n",
      "train loss:0.038404767087722676\n",
      "train loss:0.035831550748145265\n",
      "train loss:0.030589009912087968\n",
      "train loss:0.03652313424948219\n",
      "train loss:0.06266912539936614\n",
      "train loss:0.032183383819440706\n",
      "train loss:0.05666101673464269\n",
      "train loss:0.04772193100396228\n",
      "train loss:0.022745638374068664\n",
      "train loss:0.05011200265941082\n",
      "train loss:0.05160748131558969\n",
      "train loss:0.02067186689314418\n",
      "train loss:0.037029597906412796\n",
      "train loss:0.039844310675536135\n",
      "train loss:0.02206221886967341\n",
      "train loss:0.01917215722592321\n",
      "train loss:0.0732879393609243\n",
      "train loss:0.08632433952372773\n",
      "train loss:0.05855358947784409\n",
      "train loss:0.05988981437365855\n",
      "train loss:0.02140240239441372\n",
      "train loss:0.022693790713284194\n",
      "train loss:0.012618165702541546\n",
      "train loss:0.020353442123677446\n",
      "train loss:0.009328101725470883\n",
      "train loss:0.07571902897020907\n",
      "train loss:0.030451685414010044\n",
      "train loss:0.028555889118285268\n",
      "train loss:0.04753902177009078\n",
      "train loss:0.06030469045534802\n",
      "=== epoch:17, train acc:0.987, test acc:0.954 ===\n",
      "train loss:0.017367572467433586\n",
      "train loss:0.047464560092919925\n",
      "train loss:0.03304243185616036\n",
      "train loss:0.03846204312829252\n",
      "train loss:0.028918501260159237\n",
      "train loss:0.013627686856276655\n",
      "train loss:0.011034899092542952\n",
      "train loss:0.03337726275019593\n",
      "train loss:0.0511583518998158\n",
      "train loss:0.05069294098314686\n",
      "train loss:0.021218634580276684\n",
      "train loss:0.023018992131751192\n",
      "train loss:0.016893643294116377\n",
      "train loss:0.08261570951610425\n",
      "train loss:0.04733661654446395\n",
      "train loss:0.03433594297928331\n",
      "train loss:0.014869703006719803\n",
      "train loss:0.03484400496499818\n",
      "train loss:0.0495414812009081\n",
      "train loss:0.0322044364413765\n",
      "train loss:0.060373018986741016\n",
      "train loss:0.06801565905447815\n",
      "train loss:0.025527336867302054\n",
      "train loss:0.023214146470299273\n",
      "train loss:0.023074984506880794\n",
      "train loss:0.013780006111183195\n",
      "train loss:0.024709457570088692\n",
      "train loss:0.04281697449376466\n",
      "train loss:0.021626380105785013\n",
      "train loss:0.042106548071200185\n",
      "train loss:0.02123971454371168\n",
      "train loss:0.030010554613792668\n",
      "train loss:0.034998522811152954\n",
      "train loss:0.029999765557300155\n",
      "train loss:0.019400626226493856\n",
      "train loss:0.02663017995247604\n",
      "train loss:0.04028563022176204\n",
      "train loss:0.025437274470534506\n",
      "train loss:0.029453081039623345\n",
      "train loss:0.04808777734958369\n",
      "train loss:0.015599939728614294\n",
      "train loss:0.021276470819895338\n",
      "train loss:0.02121572953979754\n",
      "train loss:0.04752493794420604\n",
      "train loss:0.042187705905166205\n",
      "train loss:0.11482474443796394\n",
      "train loss:0.03263267094366488\n",
      "train loss:0.034436140196537096\n",
      "train loss:0.02480007130970715\n",
      "train loss:0.02083190483896653\n",
      "=== epoch:18, train acc:0.983, test acc:0.956 ===\n",
      "train loss:0.048969897530751184\n",
      "train loss:0.036598726914718924\n",
      "train loss:0.04475012718448556\n",
      "train loss:0.037418273331874176\n",
      "train loss:0.01345698524414839\n",
      "train loss:0.04003176731087737\n",
      "train loss:0.02879847860661995\n",
      "train loss:0.049039078613246675\n",
      "train loss:0.0768605337062145\n",
      "train loss:0.02657806139382669\n",
      "train loss:0.04057965467398986\n",
      "train loss:0.015528427190884176\n",
      "train loss:0.018416438953138905\n",
      "train loss:0.04152859583139429\n",
      "train loss:0.027180218662078495\n",
      "train loss:0.05354982534690781\n",
      "train loss:0.01860581071318367\n",
      "train loss:0.03365944009694102\n",
      "train loss:0.014361685302449965\n",
      "train loss:0.02634204642408211\n",
      "train loss:0.01491772875980887\n",
      "train loss:0.040573058571436456\n",
      "train loss:0.04102173817232518\n",
      "train loss:0.025651759942486748\n",
      "train loss:0.03476859976725802\n",
      "train loss:0.014430216096810803\n",
      "train loss:0.013643799263712831\n",
      "train loss:0.015293267153128455\n",
      "train loss:0.024925042094898226\n",
      "train loss:0.04868941042521117\n",
      "train loss:0.05124826014840385\n",
      "train loss:0.019170431548520286\n",
      "train loss:0.031044216625156346\n",
      "train loss:0.027808464438355242\n",
      "train loss:0.026101354924102184\n",
      "train loss:0.03916216814875541\n",
      "train loss:0.034245092890442023\n",
      "train loss:0.027579688636681986\n",
      "train loss:0.03930219299451017\n",
      "train loss:0.03042122712047768\n",
      "train loss:0.0101538451364534\n",
      "train loss:0.029804606718855015\n",
      "train loss:0.01572567907480131\n",
      "train loss:0.03479022888735983\n",
      "train loss:0.052358513416907664\n",
      "train loss:0.034204906400882334\n",
      "train loss:0.03861431626766667\n",
      "train loss:0.020612676657589753\n",
      "train loss:0.011455367491664568\n",
      "train loss:0.010577008022934669\n",
      "=== epoch:19, train acc:0.99, test acc:0.956 ===\n",
      "train loss:0.009231055936284906\n",
      "train loss:0.025459292863300714\n",
      "train loss:0.009863721893584644\n",
      "train loss:0.030703883427130273\n",
      "train loss:0.029414091965347597\n",
      "train loss:0.019730624165314424\n",
      "train loss:0.02161474408368337\n",
      "train loss:0.031922330652603755\n",
      "train loss:0.008677009610120991\n",
      "train loss:0.04449068218254892\n",
      "train loss:0.024080727407700772\n",
      "train loss:0.01339625314340177\n",
      "train loss:0.019140649720834953\n",
      "train loss:0.09637626389147912\n",
      "train loss:0.05327456830426664\n",
      "train loss:0.03511865305098738\n",
      "train loss:0.03406829093147356\n",
      "train loss:0.03567399961564872\n",
      "train loss:0.008574726021133957\n",
      "train loss:0.01804373520144142\n",
      "train loss:0.03756923268262585\n",
      "train loss:0.026949866318003085\n",
      "train loss:0.04695799429850434\n",
      "train loss:0.024801119231176574\n",
      "train loss:0.029858145717826433\n",
      "train loss:0.02656096534163187\n",
      "train loss:0.022663355655663263\n",
      "train loss:0.010505408274664325\n",
      "train loss:0.06803392487371022\n",
      "train loss:0.015219356690052683\n",
      "train loss:0.016790646967102578\n",
      "train loss:0.018709039490900067\n",
      "train loss:0.030060514378736577\n",
      "train loss:0.03285281600562612\n",
      "train loss:0.013405028859949845\n",
      "train loss:0.017804733668975937\n",
      "train loss:0.03410681256181734\n",
      "train loss:0.013297801772174777\n",
      "train loss:0.04281383662209259\n",
      "train loss:0.016376896620681976\n",
      "train loss:0.039401465990753934\n",
      "train loss:0.0217704346464655\n",
      "train loss:0.01897539959514073\n",
      "train loss:0.01768710904159308\n",
      "train loss:0.02766961560803414\n",
      "train loss:0.0077912265437313465\n",
      "train loss:0.023895824239803902\n",
      "train loss:0.04741791897026237\n",
      "train loss:0.012946154107355907\n",
      "train loss:0.010000899383589558\n",
      "=== epoch:20, train acc:0.99, test acc:0.948 ===\n",
      "train loss:0.017377118664660466\n",
      "train loss:0.0268901324453661\n",
      "train loss:0.011403907624145625\n",
      "train loss:0.04182451963990642\n",
      "train loss:0.013557685256126542\n",
      "train loss:0.01619702406299545\n",
      "train loss:0.008773139746220923\n",
      "train loss:0.047799703984640035\n",
      "train loss:0.030713503671246235\n",
      "train loss:0.026692085314267547\n",
      "train loss:0.008619870502575665\n",
      "train loss:0.036012968832680785\n",
      "train loss:0.04042489678480113\n",
      "train loss:0.03043154329135673\n",
      "train loss:0.023188041099862115\n",
      "train loss:0.023490736272536377\n",
      "train loss:0.030453262553262746\n",
      "train loss:0.0358946743296378\n",
      "train loss:0.014117233850916368\n",
      "train loss:0.021884909277835235\n",
      "train loss:0.020507843907834545\n",
      "train loss:0.017853036906579085\n",
      "train loss:0.023839999615441605\n",
      "train loss:0.029623060437898564\n",
      "train loss:0.024525495158701224\n",
      "train loss:0.01574465504896105\n",
      "train loss:0.017356615104893388\n",
      "train loss:0.045959936079901835\n",
      "train loss:0.024314538662206533\n",
      "train loss:0.00621450651606602\n",
      "train loss:0.02703017779960214\n",
      "train loss:0.007936020205729628\n",
      "train loss:0.016549110638212405\n",
      "train loss:0.012909127305735991\n",
      "train loss:0.013415638290036638\n",
      "train loss:0.014903756442108125\n",
      "train loss:0.02210988541295541\n",
      "train loss:0.065470609867625\n",
      "train loss:0.015298799560969054\n",
      "train loss:0.023382774075246963\n",
      "train loss:0.019277671223093297\n",
      "train loss:0.02391325701090801\n",
      "train loss:0.013456911540947478\n",
      "train loss:0.007264694079674875\n",
      "train loss:0.01758113498427798\n",
      "train loss:0.016181244781820134\n",
      "train loss:0.00997462796134777\n",
      "train loss:0.02168251785773174\n",
      "train loss:0.01587050842226856\n",
      "=============== Final Test Accuracy ===============\n",
      "test acc:0.957\n",
      "Saved Network Parameters!\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gzip\n",
    "from collections import OrderedDict\n",
    "from common.trainer import Trainer\n",
    "from simple_convnet import SimpleConvNet\n",
    "from common.layers import *\n",
    "from common.gradient import numerical_gradient\n",
    "\n",
    "# 로그 파일 설정\n",
    "log_file = 'accuracy_log2.txt'\n",
    "\n",
    "if os.path.exists(log_file):\n",
    "    os.remove(log_file)\n",
    "\n",
    "with open(log_file, 'w') as f:\n",
    "    f.write(\"MNIST 데이터셋을 로드합니다...\\n\")\n",
    "\n",
    "def load_mnist(path, kind='train'):\n",
    "    \"\"\"`path`에서 MNIST 데이터셋을 로드합니다.\"\"\"\n",
    "    labels_path = os.path.join(path, f'{kind}-labels-idx1-ubyte.gz')\n",
    "    images_path = os.path.join(path, f'{kind}-images-idx3-ubyte.gz')\n",
    "\n",
    "    with gzip.open(labels_path, 'rb') as lbpath:\n",
    "        labels = np.frombuffer(lbpath.read(), dtype=np.uint8, offset=8)\n",
    "\n",
    "    with gzip.open(images_path, 'rb') as imgpath:\n",
    "        images = np.frombuffer(imgpath.read(), dtype=np.uint8, offset=16).reshape(len(labels), 1, 28, 28)\n",
    "\n",
    "    with open(log_file, 'a') as f:\n",
    "        f.write(f\"{kind} 데이터를 로드했습니다.\\n\")\n",
    "\n",
    "    return images, labels\n",
    "\n",
    "# 데이터 읽기 (패션 MNIST)\n",
    "path = './dataset'  # 실제 데이터 파일들이 저장된 경로\n",
    "\n",
    "x_train, t_train = load_mnist(path, kind='train')\n",
    "x_test, t_test = load_mnist(path, kind='t10k')\n",
    "\n",
    "# 데이터 전처리\n",
    "x_train = x_train.astype(np.float32) / 255.0\n",
    "x_test = x_test.astype(np.float32) / 255.0\n",
    "t_train = np.eye(10)[t_train]  # 원-핫 인코딩\n",
    "t_test = np.eye(10)[t_test]    # 원-핫 인코딩\n",
    "\n",
    "# 학습 데이터 크기 줄이기 (코드 실행 속도를 빠르게 하기 위해)\n",
    "x_train, t_train = x_train[:5000], t_train[:5000]\n",
    "x_test, t_test = x_test[:1000], t_test[:1000]\n",
    "\n",
    "max_epochs = 20\n",
    "\n",
    "network = SimpleConvNet(input_dim=(1,28,28), \n",
    "                        conv_param={'filter_num': 30, 'filter_size': 5, 'pad': 0, 'stride': 1},\n",
    "                        hidden_size=100, output_size=10, weight_init_std=0.01)\n",
    "\n",
    "trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
    "                  epochs=max_epochs, mini_batch_size=100,\n",
    "                  optimizer='Adam', optimizer_param={'lr': 0.001},\n",
    "                  evaluate_sample_num_per_epoch=1000)\n",
    "trainer.train()\n",
    "\n",
    "# trainer.train() 안에서 에포크 단위로 train_acc_list와 test_acc_list를 생성했을 것입니다.\n",
    "# 이를 사용하여 로그 파일에 기록할 내용을 생성합니다.\n",
    "train_acc = trainer.train_acc_list[-1]  # 마지막 에포크의 train accuracy\n",
    "test_acc = trainer.test_acc_list[-1]    # 마지막 에포크의 test accuracy\n",
    "content = \"Epoch {}: Train accuracy: {:.4f}, Test accuracy: {:.4f}\".format(max_epochs, train_acc, test_acc)\n",
    "\n",
    "# 새로운 로그 파일 생성 후 내용 추가\n",
    "with open(log_file, 'a') as f:\n",
    "    f.write(content + '\\n')\n",
    "\n",
    "# 매개변수 보존\n",
    "network.save_params(\"params.pkl\")\n",
    "print(\"Saved Network Parameters!\")\n",
    "\n",
    "# 그래프 그리기\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(max_epochs)\n",
    "plt.plot(x, trainer.train_acc_list, marker='o', label='train', markevery=2)\n",
    "plt.plot(x, trainer.test_acc_list, marker='s', label='test', markevery=2)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163793a8-db26-44cf-a934-f0a2e7d3a4fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
