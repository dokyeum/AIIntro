{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "97d031f4-6c19-4976-8817-e65ca716f92e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.299273649777335\n",
      "=== epoch:1, train acc:0.125, test acc:0.136 ===\n",
      "train loss:2.298270266050508\n",
      "train loss:2.2934058562219284\n",
      "train loss:2.284681484801553\n",
      "train loss:2.2815775045831264\n",
      "train loss:2.2753859982307074\n",
      "train loss:2.260215443107558\n",
      "train loss:2.2544082347180123\n",
      "train loss:2.229568274273751\n",
      "train loss:2.201384701758507\n",
      "train loss:2.1604859364235898\n",
      "train loss:2.1148390490583937\n",
      "train loss:2.1067219256389778\n",
      "train loss:2.027084101535529\n",
      "train loss:2.0045245220921277\n",
      "train loss:1.9354574403071383\n",
      "train loss:1.8893485239764096\n",
      "train loss:1.7753109028087477\n",
      "train loss:1.6694301274297738\n",
      "train loss:1.698595842598026\n",
      "train loss:1.565654275241704\n",
      "train loss:1.5667393813348176\n",
      "train loss:1.4862937411568629\n",
      "train loss:1.3807951288150346\n",
      "train loss:1.3557780744296963\n",
      "train loss:1.1859505185251606\n",
      "train loss:1.1748263028897907\n",
      "train loss:1.0653203875963209\n",
      "train loss:1.0044681033931089\n",
      "train loss:0.9100471293888872\n",
      "train loss:0.8431274109526312\n",
      "train loss:0.9489761663365907\n",
      "train loss:0.7175533695429175\n",
      "train loss:0.862649655633729\n",
      "train loss:0.7353896617358493\n",
      "train loss:0.7512826005741353\n",
      "train loss:0.6859789220644396\n",
      "train loss:0.563279128001911\n",
      "train loss:0.6059858692906908\n",
      "train loss:0.5466718300210089\n",
      "train loss:0.5967981210398292\n",
      "train loss:0.6076490150410206\n",
      "train loss:0.4893767587798956\n",
      "train loss:0.6400325392921532\n",
      "train loss:0.5092068412458364\n",
      "train loss:0.4967810180557106\n",
      "train loss:0.55265565193347\n",
      "train loss:0.4858923625343738\n",
      "train loss:0.7741975094931846\n",
      "train loss:0.5710396505275104\n",
      "train loss:0.5678637880882057\n",
      "=== epoch:2, train acc:0.806, test acc:0.77 ===\n",
      "train loss:0.4413605453244984\n",
      "train loss:0.45449989525668977\n",
      "train loss:0.4454794476267851\n",
      "train loss:0.4197352602630374\n",
      "train loss:0.40190587107203607\n",
      "train loss:0.24456842590667244\n",
      "train loss:0.5349368284430642\n",
      "train loss:0.8252380145414261\n",
      "train loss:0.6308333175906217\n",
      "train loss:0.38773884927946894\n",
      "train loss:0.5485778760337124\n",
      "train loss:0.4460051231868658\n",
      "train loss:0.5091214596126745\n",
      "train loss:0.6401929830471851\n",
      "train loss:0.5521208920597909\n",
      "train loss:0.2876034829601044\n",
      "train loss:0.46854368260524076\n",
      "train loss:0.29962750507921515\n",
      "train loss:0.39024599956551226\n",
      "train loss:0.45020672193446853\n",
      "train loss:0.4529636357502846\n",
      "train loss:0.3183884688037613\n",
      "train loss:0.3250407383700968\n",
      "train loss:0.34847604535501175\n",
      "train loss:0.4120326758954803\n",
      "train loss:0.34513289745365505\n",
      "train loss:0.30806092993681616\n",
      "train loss:0.4912445302049258\n",
      "train loss:0.3813548078552414\n",
      "train loss:0.5151507111707534\n",
      "train loss:0.2549295687366931\n",
      "train loss:0.3098343534498858\n",
      "train loss:0.4585437002902545\n",
      "train loss:0.21105310057558038\n",
      "train loss:0.4057188840167136\n",
      "train loss:0.5236980827779993\n",
      "train loss:0.27504341074090527\n",
      "train loss:0.35342704851923146\n",
      "train loss:0.4361070220384277\n",
      "train loss:0.5027927417334046\n",
      "train loss:0.45598270094392523\n",
      "train loss:0.18790987787337887\n",
      "train loss:0.25922659728848874\n",
      "train loss:0.4292126155359172\n",
      "train loss:0.39759576287514065\n",
      "train loss:0.28750993617244136\n",
      "train loss:0.4800556538074647\n",
      "train loss:0.3568562209450402\n",
      "train loss:0.21495888858254283\n",
      "train loss:0.36650571284506145\n",
      "=== epoch:3, train acc:0.872, test acc:0.859 ===\n",
      "train loss:0.3709457873589114\n",
      "train loss:0.3136211050976359\n",
      "train loss:0.26055497578823905\n",
      "train loss:0.421655720076301\n",
      "train loss:0.2654141322390606\n",
      "train loss:0.3118928217255927\n",
      "train loss:0.18553104763383094\n",
      "train loss:0.37309610468458154\n",
      "train loss:0.3153055843387278\n",
      "train loss:0.2597137865540178\n",
      "train loss:0.12360428400076819\n",
      "train loss:0.2989120499788561\n",
      "train loss:0.2372510791615563\n",
      "train loss:0.4420815452628561\n",
      "train loss:0.2726939842054128\n",
      "train loss:0.3343982946832623\n",
      "train loss:0.2988748142448047\n",
      "train loss:0.30485222970627524\n",
      "train loss:0.25336125846073326\n",
      "train loss:0.1812870323444988\n",
      "train loss:0.275350551419395\n",
      "train loss:0.41186600266216966\n",
      "train loss:0.38997374087038544\n",
      "train loss:0.277381729910627\n",
      "train loss:0.2771476081791883\n",
      "train loss:0.2187658001301526\n",
      "train loss:0.4082907679575052\n",
      "train loss:0.2908544018183893\n",
      "train loss:0.21947260944110958\n",
      "train loss:0.2246996865734413\n",
      "train loss:0.47765302322606634\n",
      "train loss:0.25938037771488026\n",
      "train loss:0.34132048111116764\n",
      "train loss:0.22739584419093425\n",
      "train loss:0.5115132617146211\n",
      "train loss:0.16953393093730437\n",
      "train loss:0.2797668397128184\n",
      "train loss:0.20008786715253532\n",
      "train loss:0.2352925109512411\n",
      "train loss:0.25781850271667733\n",
      "train loss:0.23774112172087045\n",
      "train loss:0.23785080909528683\n",
      "train loss:0.2899824213187475\n",
      "train loss:0.2531788490548773\n",
      "train loss:0.28063777495769715\n",
      "train loss:0.23675293914184375\n",
      "train loss:0.2712010249466579\n",
      "train loss:0.2656301255157045\n",
      "train loss:0.2567451836941251\n",
      "train loss:0.26129372766983605\n",
      "=== epoch:4, train acc:0.909, test acc:0.9 ===\n",
      "train loss:0.24993173122344106\n",
      "train loss:0.15663428194264614\n",
      "train loss:0.3177458053015943\n",
      "train loss:0.24469326925752227\n",
      "train loss:0.31009132906960407\n",
      "train loss:0.3255105412120116\n",
      "train loss:0.24404819842853345\n",
      "train loss:0.2768265820775109\n",
      "train loss:0.18868792283913993\n",
      "train loss:0.14694175294345485\n",
      "train loss:0.20727687620697569\n",
      "train loss:0.150691213903593\n",
      "train loss:0.23896590582420538\n",
      "train loss:0.1385013812003084\n",
      "train loss:0.18694687187041226\n",
      "train loss:0.11767819647988143\n",
      "train loss:0.1672848465889133\n",
      "train loss:0.2840713389371175\n",
      "train loss:0.29136688326183124\n",
      "train loss:0.2124607611817694\n",
      "train loss:0.2081757107698722\n",
      "train loss:0.14358226065366317\n",
      "train loss:0.2097667899989088\n",
      "train loss:0.22106072337360796\n",
      "train loss:0.38580945793814997\n",
      "train loss:0.14148367839032475\n",
      "train loss:0.26380415866617635\n",
      "train loss:0.22143931128913774\n",
      "train loss:0.13821341652034771\n",
      "train loss:0.2418855200764796\n",
      "train loss:0.2540325706304274\n",
      "train loss:0.30083462271895045\n",
      "train loss:0.26948145306527166\n",
      "train loss:0.20132852906046442\n",
      "train loss:0.2877118348943126\n",
      "train loss:0.1500167205151317\n",
      "train loss:0.21910638500849638\n",
      "train loss:0.2344750609872517\n",
      "train loss:0.2622010405502856\n",
      "train loss:0.10815000991799067\n",
      "train loss:0.3403617755558799\n",
      "train loss:0.24597239462111734\n",
      "train loss:0.2352262282755717\n",
      "train loss:0.11417075524022983\n",
      "train loss:0.1620738799631067\n",
      "train loss:0.37163715458232593\n",
      "train loss:0.20834022788275713\n",
      "train loss:0.21127276424468192\n",
      "train loss:0.27434979572034907\n",
      "train loss:0.13157666346636598\n",
      "=== epoch:5, train acc:0.923, test acc:0.909 ===\n",
      "train loss:0.23879019462010045\n",
      "train loss:0.11107573420315385\n",
      "train loss:0.23312789169314044\n",
      "train loss:0.24962219593993176\n",
      "train loss:0.25990927167155875\n",
      "train loss:0.12451818753263874\n",
      "train loss:0.15443286241301277\n",
      "train loss:0.21510122551407496\n",
      "train loss:0.2174361748320776\n",
      "train loss:0.13904783190529116\n",
      "train loss:0.15768848270752892\n",
      "train loss:0.1660559639825551\n",
      "train loss:0.3505593466354561\n",
      "train loss:0.1358460118821002\n",
      "train loss:0.2978218399180325\n",
      "train loss:0.14687482234080346\n",
      "train loss:0.14891004461619742\n",
      "train loss:0.12362014097935184\n",
      "train loss:0.1731224035937636\n",
      "train loss:0.23788937661583057\n",
      "train loss:0.18099116556790754\n",
      "train loss:0.16132423178570496\n",
      "train loss:0.12635333772051072\n",
      "train loss:0.12946969151865068\n",
      "train loss:0.2613970789404964\n",
      "train loss:0.12759953478917757\n",
      "train loss:0.13184659572981983\n",
      "train loss:0.19497316449424099\n",
      "train loss:0.22701749890381012\n",
      "train loss:0.12253074512119014\n",
      "train loss:0.13049235707538054\n",
      "train loss:0.219120996220055\n",
      "train loss:0.2206186062708462\n",
      "train loss:0.14255531771271107\n",
      "train loss:0.10895530087876003\n",
      "train loss:0.26472258218717554\n",
      "train loss:0.22576426330389876\n",
      "train loss:0.261374464588336\n",
      "train loss:0.07962210723905236\n",
      "train loss:0.16792236353652895\n",
      "train loss:0.09211334277051617\n",
      "train loss:0.18553594403986837\n",
      "train loss:0.12381032202842647\n",
      "train loss:0.1381609400968253\n",
      "train loss:0.2052392235112075\n",
      "train loss:0.09490962884592599\n",
      "train loss:0.25922772239841196\n",
      "train loss:0.21229996436413134\n",
      "train loss:0.08914715590198066\n",
      "train loss:0.16438489161910178\n",
      "=== epoch:6, train acc:0.941, test acc:0.92 ===\n",
      "train loss:0.2310823493226213\n",
      "train loss:0.10056287124695462\n",
      "train loss:0.21433590233013844\n",
      "train loss:0.13850348930227055\n",
      "train loss:0.1288142902049688\n",
      "train loss:0.19054204182691578\n",
      "train loss:0.1989695810522478\n",
      "train loss:0.21188811805422364\n",
      "train loss:0.2901208759025278\n",
      "train loss:0.1574662880030203\n",
      "train loss:0.09685271463507471\n",
      "train loss:0.2458737101112022\n",
      "train loss:0.19746782779979657\n",
      "train loss:0.17664958783476728\n",
      "train loss:0.12349627416880854\n",
      "train loss:0.14560026104054008\n",
      "train loss:0.1312135700265845\n",
      "train loss:0.14280261017080517\n",
      "train loss:0.19785504340352206\n",
      "train loss:0.2248622513540971\n",
      "train loss:0.16633058531623376\n",
      "train loss:0.19373068967295395\n",
      "train loss:0.10965900806928718\n",
      "train loss:0.14317046615382337\n",
      "train loss:0.17268597332965677\n",
      "train loss:0.130845819072294\n",
      "train loss:0.10209759633662774\n",
      "train loss:0.15611484399730535\n",
      "train loss:0.1544157514721251\n",
      "train loss:0.11710951458791\n",
      "train loss:0.1313652044079133\n",
      "train loss:0.08242326326091887\n",
      "train loss:0.16978478465538172\n",
      "train loss:0.18372161101641768\n",
      "train loss:0.10281428437204797\n",
      "train loss:0.2616458525584988\n",
      "train loss:0.16902765756334978\n",
      "train loss:0.11875326242530457\n",
      "train loss:0.09974031838909761\n",
      "train loss:0.1604525980374628\n",
      "train loss:0.1922476354514986\n",
      "train loss:0.3119856080719234\n",
      "train loss:0.12266957517398787\n",
      "train loss:0.16087640802987022\n",
      "train loss:0.08575265672849294\n",
      "train loss:0.23994954109304012\n",
      "train loss:0.08389478492300452\n",
      "train loss:0.13658342199004797\n",
      "train loss:0.19219160191407691\n",
      "train loss:0.09570167803023462\n",
      "=== epoch:7, train acc:0.937, test acc:0.927 ===\n",
      "train loss:0.0826060111857746\n",
      "train loss:0.11586523214569076\n",
      "train loss:0.18294379709019232\n",
      "train loss:0.13541956977104197\n",
      "train loss:0.21124626933241117\n",
      "train loss:0.1849676034348067\n",
      "train loss:0.16369750946378445\n",
      "train loss:0.10921781932802842\n",
      "train loss:0.10795618207182574\n",
      "train loss:0.13051765707349555\n",
      "train loss:0.08746889978407109\n",
      "train loss:0.13123860427664227\n",
      "train loss:0.1545511512569239\n",
      "train loss:0.14430665729384096\n",
      "train loss:0.11979496958683206\n",
      "train loss:0.1046295482125048\n",
      "train loss:0.2407397020012071\n",
      "train loss:0.07839860334145977\n",
      "train loss:0.15498415296864956\n",
      "train loss:0.15426284059443862\n",
      "train loss:0.1315293455706819\n",
      "train loss:0.08587270154004223\n",
      "train loss:0.15668499578153944\n",
      "train loss:0.12311751204071948\n",
      "train loss:0.12498998258924889\n",
      "train loss:0.09661592346631767\n",
      "train loss:0.25566040315336486\n",
      "train loss:0.13456644450126848\n",
      "train loss:0.13091315440665421\n",
      "train loss:0.11280572165501032\n",
      "train loss:0.18768066369276665\n",
      "train loss:0.18303254295088306\n",
      "train loss:0.17865781947511788\n",
      "train loss:0.20178046550467016\n",
      "train loss:0.1682652039403203\n",
      "train loss:0.07311998649110713\n",
      "train loss:0.1607737620028452\n",
      "train loss:0.07336331257487588\n",
      "train loss:0.13085250181397734\n",
      "train loss:0.16540981257293633\n",
      "train loss:0.09931772980431618\n",
      "train loss:0.11446165747535492\n",
      "train loss:0.12746773985173063\n",
      "train loss:0.1382914556037912\n",
      "train loss:0.11274574418850705\n",
      "train loss:0.10157584031781709\n",
      "train loss:0.24284027321706592\n",
      "train loss:0.09942932481218822\n",
      "train loss:0.18793070039446516\n",
      "train loss:0.07099607660926355\n",
      "=== epoch:8, train acc:0.953, test acc:0.943 ===\n",
      "train loss:0.043057744096843355\n",
      "train loss:0.059325489228774744\n",
      "train loss:0.15764644908681227\n",
      "train loss:0.0793332995412188\n",
      "train loss:0.060256748608435544\n",
      "train loss:0.13233452862146827\n",
      "train loss:0.09549775161526482\n",
      "train loss:0.16412368856337647\n",
      "train loss:0.06239838438366675\n",
      "train loss:0.09599509088153656\n",
      "train loss:0.14518626308025057\n",
      "train loss:0.06844983358571428\n",
      "train loss:0.10569820611316227\n",
      "train loss:0.10199720373836094\n",
      "train loss:0.07984605856771163\n",
      "train loss:0.11774219511107611\n",
      "train loss:0.1335765792284324\n",
      "train loss:0.0614127285630998\n",
      "train loss:0.11667045876102788\n",
      "train loss:0.08438605968708224\n",
      "train loss:0.09717833137349448\n",
      "train loss:0.14787286351250117\n",
      "train loss:0.050829163080478905\n",
      "train loss:0.09908004158503361\n",
      "train loss:0.16060274440504022\n",
      "train loss:0.10384993780019842\n",
      "train loss:0.05173513211631628\n",
      "train loss:0.05234250523506479\n",
      "train loss:0.1438041490847108\n",
      "train loss:0.09201926824414879\n",
      "train loss:0.05843698480715773\n",
      "train loss:0.0850045665230259\n",
      "train loss:0.08121045191039\n",
      "train loss:0.06520112832083881\n",
      "train loss:0.0772285399384712\n",
      "train loss:0.0887303525147917\n",
      "train loss:0.13115687301863263\n",
      "train loss:0.10850177082415362\n",
      "train loss:0.09720811196372738\n",
      "train loss:0.04366303577766027\n",
      "train loss:0.08220639412653115\n",
      "train loss:0.054219912711119894\n",
      "train loss:0.07634915843397977\n",
      "train loss:0.05227474981660074\n",
      "train loss:0.06916568830940757\n",
      "train loss:0.10322690172760518\n",
      "train loss:0.03848330390435053\n",
      "train loss:0.1651418571055242\n",
      "train loss:0.08425948874756056\n",
      "train loss:0.07064822267147575\n",
      "=== epoch:9, train acc:0.962, test acc:0.938 ===\n",
      "train loss:0.06301313239949698\n",
      "train loss:0.05529273210942955\n",
      "train loss:0.19619348318111665\n",
      "train loss:0.12148321065106177\n",
      "train loss:0.12465620664200537\n",
      "train loss:0.15334870479140872\n",
      "train loss:0.04482812506968063\n",
      "train loss:0.08787536486797777\n",
      "train loss:0.0588551938528928\n",
      "train loss:0.14287317642729594\n",
      "train loss:0.10680228216444158\n",
      "train loss:0.10985211320447184\n",
      "train loss:0.07179897170824007\n",
      "train loss:0.07097801864775147\n",
      "train loss:0.04970584288525596\n",
      "train loss:0.09939575234265391\n",
      "train loss:0.07649735251861656\n",
      "train loss:0.08301486090741594\n",
      "train loss:0.1737996901181449\n",
      "train loss:0.07694700887148581\n",
      "train loss:0.05840421415177231\n",
      "train loss:0.10010678219920015\n",
      "train loss:0.10961010892451782\n",
      "train loss:0.1070814104425434\n",
      "train loss:0.10093534250857322\n",
      "train loss:0.07305152113539376\n",
      "train loss:0.02677709676064361\n",
      "train loss:0.11514444768719885\n",
      "train loss:0.08313697525632793\n",
      "train loss:0.12574742956063728\n",
      "train loss:0.10484075188861308\n",
      "train loss:0.06772233146256983\n",
      "train loss:0.05832291782342807\n",
      "train loss:0.11427573675708215\n",
      "train loss:0.06594161027111996\n",
      "train loss:0.09504410952271422\n",
      "train loss:0.04627274658278118\n",
      "train loss:0.08835003196324553\n",
      "train loss:0.048258966067849\n",
      "train loss:0.037288092827275514\n",
      "train loss:0.06840818398302488\n",
      "train loss:0.10599276892605922\n",
      "train loss:0.13136279189674352\n",
      "train loss:0.1484129093475438\n",
      "train loss:0.08570780093678432\n",
      "train loss:0.08548231081228624\n",
      "train loss:0.06534176813742702\n",
      "train loss:0.09479122554442718\n",
      "train loss:0.027765843111467507\n",
      "train loss:0.08251643475343991\n",
      "=== epoch:10, train acc:0.969, test acc:0.948 ===\n",
      "train loss:0.06913648065384265\n",
      "train loss:0.04338895443660586\n",
      "train loss:0.03531932648385441\n",
      "train loss:0.04531409201225722\n",
      "train loss:0.1590676329288935\n",
      "train loss:0.117721913636221\n",
      "train loss:0.14262732279496912\n",
      "train loss:0.08849295863272316\n",
      "train loss:0.05449937106473291\n",
      "train loss:0.034537474553450756\n",
      "train loss:0.1666161648961897\n",
      "train loss:0.056642808136061006\n",
      "train loss:0.04835052255614384\n",
      "train loss:0.11506893588068019\n",
      "train loss:0.04870694509536684\n",
      "train loss:0.05450329496428208\n",
      "train loss:0.041915697509146976\n",
      "train loss:0.06526591234031826\n",
      "train loss:0.10288866480459574\n",
      "train loss:0.028884827688435793\n",
      "train loss:0.04082402704669822\n",
      "train loss:0.12202386554284067\n",
      "train loss:0.08005624562431017\n",
      "train loss:0.08644974139123013\n",
      "train loss:0.13370042335874466\n",
      "train loss:0.05757590266221294\n",
      "train loss:0.051652165388350026\n",
      "train loss:0.029845441592933605\n",
      "train loss:0.05861238396279388\n",
      "train loss:0.10263593859511541\n",
      "train loss:0.05276874392292788\n",
      "train loss:0.061018732013244016\n",
      "train loss:0.1802617608249982\n",
      "train loss:0.0466284495122267\n",
      "train loss:0.07260780449723223\n",
      "train loss:0.1447160325021293\n",
      "train loss:0.06087552096591686\n",
      "train loss:0.07225738127344394\n",
      "train loss:0.1840024868507911\n",
      "train loss:0.11270196375558057\n",
      "train loss:0.0615074750665058\n",
      "train loss:0.1292875389709897\n",
      "train loss:0.12276701330447587\n",
      "train loss:0.06141927408282524\n",
      "train loss:0.08285123108496244\n",
      "train loss:0.0633760292587089\n",
      "train loss:0.09462384579981231\n",
      "train loss:0.07569507011963618\n",
      "train loss:0.057779775452704955\n",
      "train loss:0.03069034349568838\n",
      "=== epoch:11, train acc:0.97, test acc:0.947 ===\n",
      "train loss:0.035423546168301\n",
      "train loss:0.07600701213686198\n",
      "train loss:0.10825107493146875\n",
      "train loss:0.07232478970113668\n",
      "train loss:0.05259355194896945\n",
      "train loss:0.08312612010186403\n",
      "train loss:0.03850415431346854\n",
      "train loss:0.16040609175867004\n",
      "train loss:0.06775194387685243\n",
      "train loss:0.09827269105144743\n",
      "train loss:0.03682294299030196\n",
      "train loss:0.04649255628526992\n",
      "train loss:0.037722554207314116\n",
      "train loss:0.1009306510889462\n",
      "train loss:0.04804977738488232\n",
      "train loss:0.03743410108857653\n",
      "train loss:0.1612740755522024\n",
      "train loss:0.03164760747320416\n",
      "train loss:0.05099473069119249\n",
      "train loss:0.08601242668536146\n",
      "train loss:0.07935589548052996\n",
      "train loss:0.06842490009518892\n",
      "train loss:0.06341865657288871\n",
      "train loss:0.06427186229061388\n",
      "train loss:0.05159897757027032\n",
      "train loss:0.03199560852856162\n",
      "train loss:0.11246093911222464\n",
      "train loss:0.10518646328137154\n",
      "train loss:0.035326310199480015\n",
      "train loss:0.05728832436989287\n",
      "train loss:0.057478148985107384\n",
      "train loss:0.05986547474584078\n",
      "train loss:0.03996523258084796\n",
      "train loss:0.09509192585082916\n",
      "train loss:0.04696697000945305\n",
      "train loss:0.05051016818566672\n",
      "train loss:0.11022363720255088\n",
      "train loss:0.027906799400803962\n",
      "train loss:0.18636509028871426\n",
      "train loss:0.037490861813288215\n",
      "train loss:0.09239297718903415\n",
      "train loss:0.017237353677384555\n",
      "train loss:0.09355205443159799\n",
      "train loss:0.035179100640835716\n",
      "train loss:0.051388524040447595\n",
      "train loss:0.154246291135964\n",
      "train loss:0.05882830213442862\n",
      "train loss:0.10321458859500104\n",
      "train loss:0.030227661365983965\n",
      "train loss:0.031192425189495606\n",
      "=== epoch:12, train acc:0.976, test acc:0.951 ===\n",
      "train loss:0.03382853284013001\n",
      "train loss:0.05862072530687835\n",
      "train loss:0.1684904818895677\n",
      "train loss:0.053261216807735046\n",
      "train loss:0.1106717951534894\n",
      "train loss:0.07574769447148713\n",
      "train loss:0.05308064391142242\n",
      "train loss:0.08701020555568122\n",
      "train loss:0.08394517563793005\n",
      "train loss:0.04939763695910262\n",
      "train loss:0.07456549096814863\n",
      "train loss:0.07615297963144861\n",
      "train loss:0.04257019321174447\n",
      "train loss:0.11354023807894809\n",
      "train loss:0.10495024895910347\n",
      "train loss:0.04026846477115008\n",
      "train loss:0.05966979911355092\n",
      "train loss:0.039434033313770864\n",
      "train loss:0.05757373776033727\n",
      "train loss:0.0727981803364936\n",
      "train loss:0.040229310648198006\n",
      "train loss:0.03525385969382894\n",
      "train loss:0.041542213122426286\n",
      "train loss:0.015055842224348373\n",
      "train loss:0.0792089764230351\n",
      "train loss:0.04320268599292632\n",
      "train loss:0.10523523292833906\n",
      "train loss:0.04052014545622133\n",
      "train loss:0.028066574493526633\n",
      "train loss:0.08486783653508677\n",
      "train loss:0.09328152430528007\n",
      "train loss:0.03280173157942647\n",
      "train loss:0.019806785947718075\n",
      "train loss:0.08575613848248906\n",
      "train loss:0.03203454187339784\n",
      "train loss:0.027429902027248546\n",
      "train loss:0.07899959377360967\n",
      "train loss:0.06898460356391033\n",
      "train loss:0.058269273298706924\n",
      "train loss:0.02279712756102232\n",
      "train loss:0.05610379512490631\n",
      "train loss:0.05120870712822654\n",
      "train loss:0.062409390219439545\n",
      "train loss:0.052345738480348764\n",
      "train loss:0.09589741196943802\n",
      "train loss:0.06785135208676764\n",
      "train loss:0.030721049953926848\n",
      "train loss:0.015434620877370622\n",
      "train loss:0.02435993842663578\n",
      "train loss:0.06188704489003553\n",
      "=== epoch:13, train acc:0.983, test acc:0.959 ===\n",
      "train loss:0.06266154956016391\n",
      "train loss:0.017425803699367373\n",
      "train loss:0.052379725981223084\n",
      "train loss:0.031774707496678885\n",
      "train loss:0.0374401541161814\n",
      "train loss:0.03679191072706281\n",
      "train loss:0.047198641106090185\n",
      "train loss:0.026085608218236328\n",
      "train loss:0.056265784360867495\n",
      "train loss:0.03254288437838424\n",
      "train loss:0.04783605926869737\n",
      "train loss:0.08331346979892426\n",
      "train loss:0.06382943357390232\n",
      "train loss:0.023928583623692372\n",
      "train loss:0.05258477317699408\n",
      "train loss:0.04704218122517262\n",
      "train loss:0.11492593301544134\n",
      "train loss:0.030976023299622832\n",
      "train loss:0.02808999852342188\n",
      "train loss:0.018747228004485722\n",
      "train loss:0.07823284336771354\n",
      "train loss:0.06900001646899216\n",
      "train loss:0.08837107123903053\n",
      "train loss:0.0728732110234243\n",
      "train loss:0.019174845529012097\n",
      "train loss:0.02706799541527409\n",
      "train loss:0.055185909218527146\n",
      "train loss:0.1125452541583969\n",
      "train loss:0.06046028316141476\n",
      "train loss:0.022778687806132635\n",
      "train loss:0.04211155747723469\n",
      "train loss:0.10677096453239945\n",
      "train loss:0.014112138628771097\n",
      "train loss:0.05197597101513703\n",
      "train loss:0.06306415672045165\n",
      "train loss:0.049514580989990196\n",
      "train loss:0.040601952126069873\n",
      "train loss:0.03228384583346542\n",
      "train loss:0.09715297420563664\n",
      "train loss:0.02095546796107779\n",
      "train loss:0.02950588213285086\n",
      "train loss:0.028874244110468066\n",
      "train loss:0.043727098150660934\n",
      "train loss:0.058436278360068136\n",
      "train loss:0.0534144520628398\n",
      "train loss:0.02845881600724171\n",
      "train loss:0.08868417342744146\n",
      "train loss:0.046687431131851025\n",
      "train loss:0.1338872784813513\n",
      "train loss:0.08576221172007266\n",
      "=== epoch:14, train acc:0.98, test acc:0.96 ===\n",
      "train loss:0.04156630495283755\n",
      "train loss:0.010159686946135109\n",
      "train loss:0.06385771887953914\n",
      "train loss:0.014091506506181493\n",
      "train loss:0.05424845854250135\n",
      "train loss:0.048104388168494806\n",
      "train loss:0.036109763906757186\n",
      "train loss:0.011978946764566765\n",
      "train loss:0.029801215441522956\n",
      "train loss:0.021236381516091668\n",
      "train loss:0.02456931422981944\n",
      "train loss:0.0353858842864233\n",
      "train loss:0.05356961732994949\n",
      "train loss:0.017199044918665144\n",
      "train loss:0.10972163557364162\n",
      "train loss:0.011402165460305303\n",
      "train loss:0.08292723617630951\n",
      "train loss:0.08072926596544018\n",
      "train loss:0.020508633220698756\n",
      "train loss:0.0787191793835411\n",
      "train loss:0.02455638937659087\n",
      "train loss:0.023202840338650787\n",
      "train loss:0.036735136335431906\n",
      "train loss:0.02418851216420054\n",
      "train loss:0.035225533588783095\n",
      "train loss:0.0706355901151424\n",
      "train loss:0.02503609469311055\n",
      "train loss:0.04401201468551094\n",
      "train loss:0.023409772295286766\n",
      "train loss:0.10559092357003212\n",
      "train loss:0.029798546891536125\n",
      "train loss:0.04393129360264987\n",
      "train loss:0.015045649777568943\n",
      "train loss:0.040690658056609104\n",
      "train loss:0.038888106606641076\n",
      "train loss:0.030757396458760897\n",
      "train loss:0.0504641959832495\n",
      "train loss:0.03903555062390964\n",
      "train loss:0.019283694667332037\n",
      "train loss:0.03149619210690284\n",
      "train loss:0.022806265919991967\n",
      "train loss:0.053885923053016614\n",
      "train loss:0.07517335484561044\n",
      "train loss:0.06421465277230098\n",
      "train loss:0.08585407749762722\n",
      "train loss:0.09950868570475381\n",
      "train loss:0.01781723240366927\n",
      "train loss:0.020410428201302632\n",
      "train loss:0.03228750653339194\n",
      "train loss:0.035965394918945286\n",
      "=== epoch:15, train acc:0.989, test acc:0.958 ===\n",
      "train loss:0.035351928098773244\n",
      "train loss:0.048417854216441125\n",
      "train loss:0.057590237229096015\n",
      "train loss:0.017490083394334267\n",
      "train loss:0.01231358427657671\n",
      "train loss:0.0729584123704615\n",
      "train loss:0.020635712626285167\n",
      "train loss:0.03130707071220757\n",
      "train loss:0.04515495848509941\n",
      "train loss:0.040825131057760304\n",
      "train loss:0.04322513595808312\n",
      "train loss:0.015051483505162786\n",
      "train loss:0.015615664899535357\n",
      "train loss:0.03243960034038341\n",
      "train loss:0.032495368514381597\n",
      "train loss:0.061097441009064016\n",
      "train loss:0.03495350408432754\n",
      "train loss:0.08585722095301426\n",
      "train loss:0.015411166514749886\n",
      "train loss:0.03484947681208121\n",
      "train loss:0.030323112663160748\n",
      "train loss:0.10584857333686092\n",
      "train loss:0.060072849967774414\n",
      "train loss:0.03999960441244829\n",
      "train loss:0.07535770028280823\n",
      "train loss:0.0215972292978695\n",
      "train loss:0.019464429994617453\n",
      "train loss:0.019227564153699854\n",
      "train loss:0.05276998025429136\n",
      "train loss:0.03203380057284131\n",
      "train loss:0.021597251295964914\n",
      "train loss:0.03943781701822344\n",
      "train loss:0.02779163666039191\n",
      "train loss:0.028384263967332592\n",
      "train loss:0.07428543913432414\n",
      "train loss:0.017473232052911805\n",
      "train loss:0.011704611807974528\n",
      "train loss:0.04191665722262619\n",
      "train loss:0.07114960597057861\n",
      "train loss:0.0259561991340934\n",
      "train loss:0.02660689226174776\n",
      "train loss:0.0463725171400385\n",
      "train loss:0.0249313501188656\n",
      "train loss:0.05412695413838053\n",
      "train loss:0.01697440519115642\n",
      "train loss:0.026771661131551036\n",
      "train loss:0.013852583883031616\n",
      "train loss:0.027897657319563524\n",
      "train loss:0.03207807794375715\n",
      "train loss:0.01961870618366901\n",
      "=== epoch:16, train acc:0.992, test acc:0.959 ===\n",
      "train loss:0.045198634912506266\n",
      "train loss:0.015843470548129564\n",
      "train loss:0.022801605961038484\n",
      "train loss:0.0253321845084475\n",
      "train loss:0.01743694088718107\n",
      "train loss:0.021900200579418706\n",
      "train loss:0.017735009421124926\n",
      "train loss:0.019379294434683267\n",
      "train loss:0.015373870131776306\n",
      "train loss:0.04945354428557307\n",
      "train loss:0.019433144053142417\n",
      "train loss:0.05040462944151126\n",
      "train loss:0.04083448595088393\n",
      "train loss:0.03085118604777652\n",
      "train loss:0.015462050957044672\n",
      "train loss:0.04832837632571657\n",
      "train loss:0.007560791404537204\n",
      "train loss:0.028667566032887563\n",
      "train loss:0.050223077082428055\n",
      "train loss:0.042722512108635716\n",
      "train loss:0.013287751507553397\n",
      "train loss:0.030917561917536106\n",
      "train loss:0.018757653105123097\n",
      "train loss:0.015280444669451594\n",
      "train loss:0.07346245740286382\n",
      "train loss:0.0507706683839664\n",
      "train loss:0.019986769693400665\n",
      "train loss:0.033556748297083515\n",
      "train loss:0.03922450242116923\n",
      "train loss:0.027697573706022657\n",
      "train loss:0.03309701515426832\n",
      "train loss:0.025019602676110565\n",
      "train loss:0.018285062753683856\n",
      "train loss:0.009761488927186509\n",
      "train loss:0.07031910663212458\n",
      "train loss:0.041063068572531176\n",
      "train loss:0.04370742653155336\n",
      "train loss:0.04785819844169554\n",
      "train loss:0.05715702280896868\n",
      "train loss:0.02620632449100925\n",
      "train loss:0.01910927472434965\n",
      "train loss:0.01778789049473215\n",
      "train loss:0.025329738298591288\n",
      "train loss:0.05017814789064884\n",
      "train loss:0.014661421837012904\n",
      "train loss:0.04698698943884998\n",
      "train loss:0.005449120790101174\n",
      "train loss:0.03656983650151648\n",
      "train loss:0.021515968551246697\n",
      "train loss:0.02707062157120417\n",
      "=== epoch:17, train acc:0.99, test acc:0.959 ===\n",
      "train loss:0.025773764893604435\n",
      "train loss:0.02648157690154213\n",
      "train loss:0.006865524045569376\n",
      "train loss:0.12886292974971103\n",
      "train loss:0.0427708404585449\n",
      "train loss:0.02044233118679106\n",
      "train loss:0.007340114796671749\n",
      "train loss:0.008690246946548637\n",
      "train loss:0.024583652628149332\n",
      "train loss:0.07192238127938802\n",
      "train loss:0.027258278818710683\n",
      "train loss:0.07264484934079121\n",
      "train loss:0.021797940407401618\n",
      "train loss:0.05033303899679051\n",
      "train loss:0.02477970144113146\n",
      "train loss:0.027758461111593757\n",
      "train loss:0.010828178951156849\n",
      "train loss:0.023723074337882354\n",
      "train loss:0.019414568485147124\n",
      "train loss:0.023361434262723826\n",
      "train loss:0.016721979607941445\n",
      "train loss:0.02331651968665344\n",
      "train loss:0.026681328974678988\n",
      "train loss:0.018277373052043216\n",
      "train loss:0.020525620243404163\n",
      "train loss:0.06390490419278008\n",
      "train loss:0.023644542054871147\n",
      "train loss:0.01566924672159413\n",
      "train loss:0.047158914354797084\n",
      "train loss:0.03061099658678079\n",
      "train loss:0.02727998876945353\n",
      "train loss:0.02717644974857228\n",
      "train loss:0.016222097282889734\n",
      "train loss:0.041916668850634095\n",
      "train loss:0.0073093586926258934\n",
      "train loss:0.034015109664905105\n",
      "train loss:0.023313340096766003\n",
      "train loss:0.03036746499549108\n",
      "train loss:0.0202002157259868\n",
      "train loss:0.006999977453587361\n",
      "train loss:0.01429889658053211\n",
      "train loss:0.0107173059490751\n",
      "train loss:0.02028134862228598\n",
      "train loss:0.019488278880290068\n",
      "train loss:0.01972590324301218\n",
      "train loss:0.015041103925762774\n",
      "train loss:0.01385111900651011\n",
      "train loss:0.010813367501118125\n",
      "train loss:0.040686790410316684\n",
      "train loss:0.006628555596093869\n",
      "=== epoch:18, train acc:0.99, test acc:0.962 ===\n",
      "train loss:0.02753343138155744\n",
      "train loss:0.008298893853856482\n",
      "train loss:0.007861222743331509\n",
      "train loss:0.009448932310875324\n",
      "train loss:0.027833296414351293\n",
      "train loss:0.019360130531895267\n",
      "train loss:0.06116364909385814\n",
      "train loss:0.056587847323887726\n",
      "train loss:0.031039611639452618\n",
      "train loss:0.04860867423527518\n",
      "train loss:0.0814367738800408\n",
      "train loss:0.060072318244791624\n",
      "train loss:0.03274427320534728\n",
      "train loss:0.03478337253521843\n",
      "train loss:0.008692011656870358\n",
      "train loss:0.02851210926631653\n",
      "train loss:0.019047798292172565\n",
      "train loss:0.040142290085564136\n",
      "train loss:0.026826304440363203\n",
      "train loss:0.019081881336080308\n",
      "train loss:0.035618820790264405\n",
      "train loss:0.011203735611074017\n",
      "train loss:0.014866734820877936\n",
      "train loss:0.010352913888194273\n",
      "train loss:0.0265762669344257\n",
      "train loss:0.033829827355494224\n",
      "train loss:0.012306402675686966\n",
      "train loss:0.031288025586614296\n",
      "train loss:0.009525238775077088\n",
      "train loss:0.03673036342227427\n",
      "train loss:0.027505012946704394\n",
      "train loss:0.010414062949998392\n",
      "train loss:0.03581495195706855\n",
      "train loss:0.03615790120461599\n",
      "train loss:0.02487501532613321\n",
      "train loss:0.039141613838056616\n",
      "train loss:0.020790145331982025\n",
      "train loss:0.014534251871753385\n",
      "train loss:0.012152435170300669\n",
      "train loss:0.02092912949365124\n",
      "train loss:0.00797171868946907\n",
      "train loss:0.031079550553379826\n",
      "train loss:0.020249565952340537\n",
      "train loss:0.01754356942606554\n",
      "train loss:0.02211158223202558\n",
      "train loss:0.011951360287310711\n",
      "train loss:0.02537373882873414\n",
      "train loss:0.051587652499089594\n",
      "train loss:0.0832110517345002\n",
      "train loss:0.01748423225906052\n",
      "=== epoch:19, train acc:0.996, test acc:0.963 ===\n",
      "train loss:0.004775462169130195\n",
      "train loss:0.008493761809896245\n",
      "train loss:0.011182521975579802\n",
      "train loss:0.017092855300601097\n",
      "train loss:0.009984092751151441\n",
      "train loss:0.016112054079668724\n",
      "train loss:0.025227359281745153\n",
      "train loss:0.016426837118851827\n",
      "train loss:0.01587903931071012\n",
      "train loss:0.007733257130981813\n",
      "train loss:0.03806169808038859\n",
      "train loss:0.028855851636701183\n",
      "train loss:0.010683780738889648\n",
      "train loss:0.022333039913127836\n",
      "train loss:0.028518882053257096\n",
      "train loss:0.013307707202557847\n",
      "train loss:0.005794846394530258\n",
      "train loss:0.009679444506161495\n",
      "train loss:0.018205916424979592\n",
      "train loss:0.0453497441020813\n",
      "train loss:0.010735837889551444\n",
      "train loss:0.02210950275665382\n",
      "train loss:0.01043157245532777\n",
      "train loss:0.010598050409826571\n",
      "train loss:0.019021192277820707\n",
      "train loss:0.00993805315348654\n",
      "train loss:0.007523809273186037\n",
      "train loss:0.014160202176090753\n",
      "train loss:0.017613824758012816\n",
      "train loss:0.01627463810784588\n",
      "train loss:0.014820981300947578\n",
      "train loss:0.014807604815449696\n",
      "train loss:0.01965427445391438\n",
      "train loss:0.03449606349301411\n",
      "train loss:0.03482502352548402\n",
      "train loss:0.011142337999366314\n",
      "train loss:0.010696202143073406\n",
      "train loss:0.009805417454432423\n",
      "train loss:0.02701184174582957\n",
      "train loss:0.035931822807741834\n",
      "train loss:0.00949263286051795\n",
      "train loss:0.006125510812317131\n",
      "train loss:0.01819082179156943\n",
      "train loss:0.02731959101898683\n",
      "train loss:0.06618565896118289\n",
      "train loss:0.02545750808424309\n",
      "train loss:0.05349321483344784\n",
      "train loss:0.014781463634775139\n",
      "train loss:0.007797687725196764\n",
      "train loss:0.029881901746366502\n",
      "=== epoch:20, train acc:0.994, test acc:0.959 ===\n",
      "train loss:0.016704756518690477\n",
      "train loss:0.03628256405013776\n",
      "train loss:0.04016875263952973\n",
      "train loss:0.012701641832086276\n",
      "train loss:0.009100605662433169\n",
      "train loss:0.013360339874325633\n",
      "train loss:0.005538845686580076\n",
      "train loss:0.011390908906959318\n",
      "train loss:0.012705826566257954\n",
      "train loss:0.007701785533761524\n",
      "train loss:0.015589788534343825\n",
      "train loss:0.06380552304219975\n",
      "train loss:0.01653211663255448\n",
      "train loss:0.018675700771515154\n",
      "train loss:0.0056631528729724386\n",
      "train loss:0.015355628328670165\n",
      "train loss:0.010157201678287632\n",
      "train loss:0.009098460077242482\n",
      "train loss:0.03158148088822209\n",
      "train loss:0.028786837674244534\n",
      "train loss:0.008791116857585834\n",
      "train loss:0.010420605242109984\n",
      "train loss:0.01389190167987629\n",
      "train loss:0.01211065503154772\n",
      "train loss:0.008621076792979626\n",
      "train loss:0.01188550300481627\n",
      "train loss:0.005810595424519215\n",
      "train loss:0.014843155338708749\n",
      "train loss:0.005513805047318502\n",
      "train loss:0.006656506621427423\n",
      "train loss:0.02268729078642697\n",
      "train loss:0.006828596684895211\n",
      "train loss:0.007072165406085037\n",
      "train loss:0.027206939501831978\n",
      "train loss:0.007242065539016761\n",
      "train loss:0.010448602893097944\n",
      "train loss:0.014686157252679667\n",
      "train loss:0.004931318742150699\n",
      "train loss:0.016223304286279515\n",
      "train loss:0.007552205944833063\n",
      "train loss:0.012945008183450658\n",
      "train loss:0.014064981532275119\n",
      "train loss:0.009973833573340593\n",
      "train loss:0.009260217271458598\n",
      "train loss:0.007755273991731359\n",
      "train loss:0.010143933526087357\n",
      "train loss:0.0098796876101432\n",
      "train loss:0.045584499651474084\n",
      "train loss:0.0074650752366733075\n",
      "=============== Final Test Accuracy ===============\n",
      "test acc:0.96\n",
      "Saved Network Parameters!\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset.mnist import load_mnist\n",
    "from common.trainer import Trainer\n",
    "from simple_convnet import SimpleConvNet\n",
    "from collections import OrderedDict\n",
    "from common.layers import *\n",
    "from common.gradient import numerical_gradient\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class SimpleConvNet:\n",
    "    \"\"\"단순한 합성곱 신경망\n",
    "    \n",
    "    conv - relu - pool - affine - relu - affine - softmax\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    input_size : 입력 크기（MNIST의 경우엔 784）\n",
    "    hidden_size_list : 각 은닉층의 뉴런 수를 담은 리스트（e.g. [100, 100, 100]）\n",
    "    output_size : 출력 크기（MNIST의 경우엔 10）\n",
    "    activation : 활성화 함수 - 'relu' 혹은 'sigmoid'\n",
    "    weight_init_std : 가중치의 표준편차 지정（e.g. 0.01）\n",
    "        'relu'나 'he'로 지정하면 'He 초깃값'으로 설정\n",
    "        'sigmoid'나 'xavier'로 지정하면 'Xavier 초깃값'으로 설정\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim=(1, 28, 28), \n",
    "                 conv_param={'filter_num':30, 'filter_size':5, 'pad':0, 'stride':1},\n",
    "                 hidden_size=100, output_size=10, weight_init_std=0.01):\n",
    "        filter_num = conv_param['filter_num']\n",
    "        filter_size = conv_param['filter_size']\n",
    "        filter_pad = conv_param['pad']\n",
    "        filter_stride = conv_param['stride']\n",
    "        input_size = input_dim[1]\n",
    "        conv_output_size = (input_size - filter_size + 2*filter_pad) / filter_stride + 1\n",
    "        pool_output_size = int(filter_num * (conv_output_size/2) * (conv_output_size/2))\n",
    "\n",
    "        # 가중치 초기화\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * \\\n",
    "                            np.random.randn(filter_num, input_dim[0], filter_size, filter_size)\n",
    "        self.params['b1'] = np.zeros(filter_num)\n",
    "        self.params['W2'] = weight_init_std * \\\n",
    "                            np.random.randn(pool_output_size, hidden_size)\n",
    "        self.params['b2'] = np.zeros(hidden_size)\n",
    "        self.params['W3'] = weight_init_std * \\\n",
    "                            np.random.randn(hidden_size, output_size)\n",
    "        self.params['b3'] = np.zeros(output_size)\n",
    "\n",
    "        # 계층 생성\n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Conv1'] = Convolution(self.params['W1'], self.params['b1'],\n",
    "                                           conv_param['stride'], conv_param['pad'])\n",
    "        self.layers['Relu1'] = Relu()\n",
    "        self.layers['Pool1'] = Pooling(pool_h=2, pool_w=2, stride=2)\n",
    "        self.layers['Affine1'] = Affine(self.params['W2'], self.params['b2'])\n",
    "        self.layers['Relu2'] = Relu()\n",
    "        self.layers['Affine2'] = Affine(self.params['W3'], self.params['b3'])\n",
    "\n",
    "        self.last_layer = SoftmaxWithLoss()\n",
    "\n",
    "    def predict(self, x):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def loss(self, x, t):\n",
    "        \"\"\"손실 함수를 구한다.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : 입력 데이터\n",
    "        t : 정답 레이블\n",
    "        \"\"\"\n",
    "        y = self.predict(x)\n",
    "        return self.last_layer.forward(y, t)\n",
    "\n",
    "    def accuracy(self, x, t, batch_size=100):\n",
    "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
    "        \n",
    "        acc = 0.0\n",
    "        \n",
    "        for i in range(int(x.shape[0] / batch_size)):\n",
    "            tx = x[i*batch_size:(i+1)*batch_size]\n",
    "            tt = t[i*batch_size:(i+1)*batch_size]\n",
    "            y = self.predict(tx)\n",
    "            y = np.argmax(y, axis=1)\n",
    "            acc += np.sum(y == tt) \n",
    "        \n",
    "        return acc / x.shape[0]\n",
    "\n",
    "    def numerical_gradient(self, x, t):\n",
    "        \"\"\"기울기를 구한다（수치미분）.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : 입력 데이터\n",
    "        t : 정답 레이블\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        각 층의 기울기를 담은 사전(dictionary) 변수\n",
    "            grads['W1']、grads['W2']、... 각 층의 가중치\n",
    "            grads['b1']、grads['b2']、... 각 층의 편향\n",
    "        \"\"\"\n",
    "        loss_w = lambda w: self.loss(x, t)\n",
    "\n",
    "        grads = {}\n",
    "        for idx in (1, 2, 3):\n",
    "            grads['W' + str(idx)] = numerical_gradient(loss_w, self.params['W' + str(idx)])\n",
    "            grads['b' + str(idx)] = numerical_gradient(loss_w, self.params['b' + str(idx)])\n",
    "\n",
    "        return grads\n",
    "\n",
    "    def gradient(self, x, t):\n",
    "        \"\"\"기울기를 구한다(오차역전파법).\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : 입력 데이터\n",
    "        t : 정답 레이블\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        각 층의 기울기를 담은 사전(dictionary) 변수\n",
    "            grads['W1']、grads['W2']、... 각 층의 가중치\n",
    "            grads['b1']、grads['b2']、... 각 층의 편향\n",
    "        \"\"\"\n",
    "        # forward\n",
    "        self.loss(x, t)\n",
    "\n",
    "        # backward\n",
    "        dout = 1\n",
    "        dout = self.last_layer.backward(dout)\n",
    "\n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "\n",
    "        # 결과 저장\n",
    "        grads = {}\n",
    "        grads['W1'], grads['b1'] = self.layers['Conv1'].dW, self.layers['Conv1'].db\n",
    "        grads['W2'], grads['b2'] = self.layers['Affine1'].dW, self.layers['Affine1'].db\n",
    "        grads['W3'], grads['b3'] = self.layers['Affine2'].dW, self.layers['Affine2'].db\n",
    "\n",
    "        return grads\n",
    "        \n",
    "    def save_params(self, file_name=\"params.pkl\"):\n",
    "        params = {}\n",
    "        for key, val in self.params.items():\n",
    "            params[key] = val\n",
    "        with open(file_name, 'wb') as f:\n",
    "            pickle.dump(params, f)\n",
    "\n",
    "    def load_params(self, file_name=\"params.pkl\"):\n",
    "        with open(file_name, 'rb') as f:\n",
    "            params = pickle.load(f)\n",
    "        for key, val in params.items():\n",
    "            self.params[key] = val\n",
    "\n",
    "        for i, key in enumerate(['Conv1', 'Affine1', 'Affine2']):\n",
    "            self.layers[key].W = self.params['W' + str(i+1)]\n",
    "            self.layers[key].b = self.params['b' + str(i+1)]\n",
    "\n",
    "# 데이터 읽기\n",
    "x_train, t_train = x_train[:5000], t_train[:5000]\n",
    "x_test, t_test = x_test[:1000], t_test[:1000]\n",
    "\n",
    "max_epochs = 20\n",
    "\n",
    "network = SimpleConvNet(input_dim=(1,28,28), \n",
    "                        conv_param = {'filter_num': 30, 'filter_size': 5, 'pad': 0, 'stride': 1},\n",
    "                        hidden_size=100, output_size=10, weight_init_std=0.01)\n",
    "                        \n",
    "trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
    "                  epochs=max_epochs, mini_batch_size=100,\n",
    "                  optimizer='Adam', optimizer_param={'lr': 0.001},\n",
    "                  evaluate_sample_num_per_epoch=1000)\n",
    "trainer.train()\n",
    "\n",
    "log_file = 'accuracy_log.txt'\n",
    "\n",
    "if os.path.exists(log_file):\n",
    "    os.remove(log_file)\n",
    "\n",
    "\n",
    "content = \"Epoch {}: Train accuracy: {:.4f}, Test accuracy: {:.4f}\".format(epoch+1, train_acc, test_acc)\n",
    "\n",
    "# 새로운 로그 파일 생성 후 내용 추가\n",
    "with open(log_file, 'w') as f:\n",
    "    f.write(content + '\\n')\n",
    "\n",
    "# 매개변수 보존\n",
    "network.save_params(\"params.pkl\")\n",
    "print(\"Saved Network Parameters!\")\n",
    "\n",
    "\n",
    "# 그래프 그리기\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(max_epochs)\n",
    "plt.plot(x, trainer.train_acc_list, marker='o', label='train', markevery=2)\n",
    "plt.plot(x, trainer.test_acc_list, marker='s', label='test', markevery=2)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f26885a-57ba-4666-9ec5-b3a90fdd9e9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1f24d3-9441-4f66-8082-4bb8a16500c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
